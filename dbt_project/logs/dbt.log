[0m10:18:02.578703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3d0843340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3cf8b2f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3cf8b2fb0>]}


============================== 10:18:02.584180 | 1ef79fe3-b3d9-4cfe-a07e-0991082e87fc ==============================
[0m10:18:02.584180 [info ] [MainThread]: Running with dbt=1.10.11
[0m10:18:02.585269 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'invocation_command': 'dbt init --skip-profile-setup', 'indirect_selection': 'eager', 'debug': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'target_path': 'None', 'use_colors': 'True', 'warn_error': 'None', 'write_json': 'True', 'version_check': 'True', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'fail_fast': 'False', 'introspect': 'True', 'empty': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'profiles_dir': '/home/bquser/.dbt', 'printer_width': '80', 'send_anonymous_usage_stats': 'True'}
[0m10:18:02.597862 [info ] [MainThread]: Creating dbt configuration folder at /home/bquser/.dbt
[0m10:18:02.599860 [debug] [MainThread]: Resource report: {"command_name": "init", "command_success": true, "command_wall_clock_time": 0.12479595, "process_in_blocks": "488", "process_kernel_time": 0.213717, "process_mem_max_rss": "115084", "process_out_blocks": "8", "process_user_time": 4.05475}
[0m10:18:02.600898 [debug] [MainThread]: Command `dbt init` succeeded at 10:18:02.600681 after 0.13 seconds
[0m10:18:02.601632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3d0843340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3cf8b2710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc3cf8b3100>]}
[0m10:18:02.602405 [debug] [MainThread]: Flushing usage events
[0m10:18:02.841867 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:18:50.823616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593fabf580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593e877f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593e877eb0>]}


============================== 10:18:50.828257 | 9c720381-e8a8-41e9-8114-6930b294b187 ==============================
[0m10:18:50.828257 [info ] [MainThread]: Running with dbt=1.10.11
[0m10:18:50.829272 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run', 'profiles_dir': '/home/bquser/.dbt', 'static_parser': 'True', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'warn_error': 'None', 'indirect_selection': 'eager', 'introspect': 'True', 'version_check': 'True', 'write_json': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'target_path': 'None', 'no_print': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80'}
[0m10:18:50.833647 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'techshop_2025'
[0m10:18:50.835340 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.107840165, "process_in_blocks": "0", "process_kernel_time": 0.143904, "process_mem_max_rss": "115088", "process_out_blocks": "8", "process_user_time": 3.979362}
[0m10:18:50.836298 [debug] [MainThread]: Command `dbt run` failed at 10:18:50.836103 after 0.11 seconds
[0m10:18:50.836939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593fabf580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593e876ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75593e8752d0>]}
[0m10:18:50.837623 [debug] [MainThread]: Flushing usage events
[0m10:18:51.037752 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:29:15.371239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768031576a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7680303cf370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7680303cf340>]}


============================== 10:29:15.379762 | 30ddbcf1-5321-43f0-9a2d-0c8b576fac84 ==============================
[0m10:29:15.379762 [info ] [MainThread]: Running with dbt=1.10.11
[0m10:29:15.381794 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'partial_parse': 'True', 'write_json': 'True', 'version_check': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'indirect_selection': 'eager', 'profiles_dir': '/home/bquser/.dbt', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'debug': 'False', 'quiet': 'False', 'empty': 'None', 'target_path': 'None', 'printer_width': '80', 'log_cache_events': 'False', 'warn_error': 'None', 'no_print': 'None', 'fail_fast': 'False', 'log_format': 'default'}
[0m10:29:15.400574 [info ] [MainThread]: dbt version: 1.10.11
[0m10:29:15.401548 [info ] [MainThread]: python version: 3.10.12
[0m10:29:15.402273 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m10:29:15.402933 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m10:29:20.972355 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m10:29:20.973334 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m10:29:20.974134 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m10:29:20.974866 [info ] [MainThread]: adapter type: bigquery
[0m10:29:20.975525 [info ] [MainThread]: adapter version: 1.10.2
[0m10:29:21.221751 [info ] [MainThread]: Configuration:
[0m10:29:21.222776 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:29:21.223534 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:29:21.224157 [info ] [MainThread]: Required dependencies:
[0m10:29:21.224844 [debug] [MainThread]: Executing "git --help"
[0m10:29:21.229592 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:29:21.230804 [debug] [MainThread]: STDERR: "b''"
[0m10:29:21.231439 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:29:21.232732 [info ] [MainThread]: Connection:
[0m10:29:21.233620 [info ] [MainThread]:   method: oauth
[0m10:29:21.234415 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m10:29:21.235147 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m10:29:21.235804 [info ] [MainThread]:   schema: staging
[0m10:29:21.236445 [info ] [MainThread]:   location: US
[0m10:29:21.237001 [info ] [MainThread]:   priority: None
[0m10:29:21.237555 [info ] [MainThread]:   maximum_bytes_billed: None
[0m10:29:21.238434 [info ] [MainThread]:   impersonate_service_account: None
[0m10:29:21.239295 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m10:29:21.240332 [info ] [MainThread]:   job_retries: 1
[0m10:29:21.241975 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m10:29:21.242874 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m10:29:21.243810 [info ] [MainThread]:   timeout_seconds: 300
[0m10:29:21.244680 [info ] [MainThread]:   client_id: None
[0m10:29:21.245523 [info ] [MainThread]:   token_uri: None
[0m10:29:21.246368 [info ] [MainThread]:   compute_region: None
[0m10:29:21.247339 [info ] [MainThread]:   dataproc_cluster_name: None
[0m10:29:21.248239 [info ] [MainThread]:   gcs_bucket: None
[0m10:29:21.248892 [info ] [MainThread]:   dataproc_batch: None
[0m10:29:21.250169 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:29:21.618639 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m10:29:21.619596 [debug] [MainThread]: On debug: select 1 as id
[0m10:29:21.620479 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:29:21.621628 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: 'Runtime Error
  Failed to authenticate with supplied credentials
  error:
  File credentials/service-account-key.json was not found.'
[0m10:29:21.622419 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m10:29:21.623017 [debug] [MainThread]: BigQuery adapter: Database Error
  Runtime Error
    Failed to authenticate with supplied credentials
    error:
    File credentials/service-account-key.json was not found.
[0m10:29:21.623943 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m10:29:21.624606 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:29:21.626002 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m10:29:21.627655 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 6.3925695, "process_in_blocks": "56", "process_kernel_time": 1.370235, "process_mem_max_rss": "370104", "process_out_blocks": "32", "process_user_time": 10.589618}
[0m10:29:21.628967 [debug] [MainThread]: Command `dbt debug` failed at 10:29:21.628728 after 6.39 seconds
[0m10:29:21.629752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768031576a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768030ef2020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768030ef3dc0>]}
[0m10:29:21.631510 [debug] [MainThread]: Flushing usage events
[0m10:29:21.909215 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:39:20.493407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e28dd3730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e27b8bf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e27b8bf10>]}


============================== 10:39:20.497983 | e105ee15-bb91-4972-8536-2e8a3608c92c ==============================
[0m10:39:20.497983 [info ] [MainThread]: Running with dbt=1.10.11
[0m10:39:20.499105 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'debug': 'False', 'profiles_dir': '/home/bquser/.dbt', 'empty': 'None', 'log_format': 'default', 'quiet': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'version_check': 'True', 'write_json': 'True', 'indirect_selection': 'eager', 'printer_width': '80'}
[0m10:39:20.511325 [info ] [MainThread]: dbt version: 1.10.11
[0m10:39:20.512168 [info ] [MainThread]: python version: 3.10.12
[0m10:39:20.512910 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m10:39:20.513546 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m10:39:23.781254 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m10:39:23.782413 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m10:39:23.783110 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m10:39:23.959173 [info ] [MainThread]: Configuration:
[0m10:39:23.960053 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m10:39:23.960761 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:39:23.961387 [info ] [MainThread]: Required dependencies:
[0m10:39:23.962103 [debug] [MainThread]: Executing "git --help"
[0m10:39:23.965665 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:39:23.966601 [debug] [MainThread]: STDERR: "b''"
[0m10:39:23.967406 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:39:23.968160 [info ] [MainThread]: Connection test skipped since no profile was found
[0m10:39:23.968814 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:39:23.969567 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "techshop_2025", target "dev" invalid: Runtime Error
    Got duplicate keys: (timeout_seconds) all map to "job_execution_timeout_seconds"


[0m10:39:23.972023 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 3.574589, "process_in_blocks": "0", "process_kernel_time": 0.442167, "process_mem_max_rss": "367616", "process_out_blocks": "16", "process_user_time": 7.559239}
[0m10:39:23.973420 [debug] [MainThread]: Command `dbt debug` failed at 10:39:23.973199 after 3.58 seconds
[0m10:39:23.974251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e28dd3730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e27a189d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782e285cb2e0>]}
[0m10:39:23.975426 [debug] [MainThread]: Flushing usage events
[0m10:39:24.230889 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:50:36.635928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x773650eeed40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77364fd279a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77364fd27940>]}


============================== 10:50:36.640728 | 06721960-021e-4da3-af4c-8870116029f9 ==============================
[0m10:50:36.640728 [info ] [MainThread]: Running with dbt=1.10.11
[0m10:50:36.641730 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'fail_fast': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'debug': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'invocation_command': 'dbt debug', 'profiles_dir': '/home/bquser/.dbt', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'write_json': 'True', 'use_colors': 'True', 'printer_width': '80', 'warn_error': 'None', 'quiet': 'False', 'no_print': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'target_path': 'None'}
[0m10:50:36.654326 [info ] [MainThread]: dbt version: 1.10.11
[0m10:50:36.655197 [info ] [MainThread]: python version: 3.10.12
[0m10:50:36.656337 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m10:50:36.657529 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m10:50:40.035219 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m10:50:40.036217 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m10:50:40.036983 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m10:50:40.037780 [info ] [MainThread]: adapter type: bigquery
[0m10:50:40.038451 [info ] [MainThread]: adapter version: 1.10.2
[0m10:50:40.214470 [info ] [MainThread]: Configuration:
[0m10:50:40.215439 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:50:40.216179 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:50:40.216806 [info ] [MainThread]: Required dependencies:
[0m10:50:40.217536 [debug] [MainThread]: Executing "git --help"
[0m10:50:40.221412 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:50:40.222250 [debug] [MainThread]: STDERR: "b''"
[0m10:50:40.223008 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:50:40.223662 [info ] [MainThread]: Connection:
[0m10:50:40.224415 [info ] [MainThread]:   method: service-account
[0m10:50:40.225009 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m10:50:40.225635 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m10:50:40.226254 [info ] [MainThread]:   schema: staging
[0m10:50:40.226915 [info ] [MainThread]:   location: US
[0m10:50:40.227533 [info ] [MainThread]:   priority: None
[0m10:50:40.228207 [info ] [MainThread]:   maximum_bytes_billed: None
[0m10:50:40.228767 [info ] [MainThread]:   impersonate_service_account: None
[0m10:50:40.229395 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m10:50:40.229983 [info ] [MainThread]:   job_retries: 1
[0m10:50:40.230640 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m10:50:40.231263 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m10:50:40.231842 [info ] [MainThread]:   timeout_seconds: 300
[0m10:50:40.232436 [info ] [MainThread]:   client_id: None
[0m10:50:40.233030 [info ] [MainThread]:   token_uri: None
[0m10:50:40.233658 [info ] [MainThread]:   compute_region: None
[0m10:50:40.234340 [info ] [MainThread]:   dataproc_cluster_name: None
[0m10:50:40.234973 [info ] [MainThread]:   gcs_bucket: None
[0m10:50:40.235615 [info ] [MainThread]:   dataproc_batch: None
[0m10:50:40.236630 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:50:40.544539 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m10:50:40.545390 [debug] [MainThread]: On debug: select 1 as id
[0m10:50:40.546092 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:50:40.546820 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: '[Errno 2] No such file or directory: '~/.dbt/credentials/dbt-service-account.json''
[0m10:50:40.547570 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m10:50:40.548201 [debug] [MainThread]: BigQuery adapter: Database Error
  [Errno 2] No such file or directory: '~/.dbt/credentials/dbt-service-account.json'
[0m10:50:40.548762 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m10:50:40.549391 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:50:40.549950 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m10:50:40.551431 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 4.012161, "process_in_blocks": "0", "process_kernel_time": 0.552567, "process_mem_max_rss": "370204", "process_out_blocks": "24", "process_user_time": 7.830252}
[0m10:50:40.552599 [debug] [MainThread]: Command `dbt debug` failed at 10:50:40.552391 after 4.01 seconds
[0m10:50:40.553331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x773650eeed40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7736506533a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x773650652050>]}
[0m10:50:40.554042 [debug] [MainThread]: Flushing usage events
[0m10:50:40.775821 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:00:32.066456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cbe6a350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cacc5b40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cacc5ae0>]}


============================== 11:00:32.071278 | 21c5ca4c-335a-409b-92cc-8a7483f9b76a ==============================
[0m11:00:32.071278 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:00:32.072438 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/bquser/.dbt', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'static_parser': 'True', 'target_path': 'None', 'fail_fast': 'False', 'printer_width': '80', 'use_colors': 'True'}
[0m11:00:32.084615 [info ] [MainThread]: dbt version: 1.10.11
[0m11:00:32.085498 [info ] [MainThread]: python version: 3.10.12
[0m11:00:32.086137 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m11:00:32.086728 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m11:00:35.466738 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m11:00:35.467943 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m11:00:35.468789 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m11:00:35.469507 [info ] [MainThread]: adapter type: bigquery
[0m11:00:35.470187 [info ] [MainThread]: adapter version: 1.10.2
[0m11:00:35.657900 [info ] [MainThread]: Configuration:
[0m11:00:35.658901 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:00:35.659585 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:00:35.660215 [info ] [MainThread]: Required dependencies:
[0m11:00:35.660833 [debug] [MainThread]: Executing "git --help"
[0m11:00:35.664720 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:00:35.666047 [debug] [MainThread]: STDERR: "b''"
[0m11:00:35.666738 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:00:35.667470 [info ] [MainThread]: Connection:
[0m11:00:35.668216 [info ] [MainThread]:   method: oauth
[0m11:00:35.668844 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m11:00:35.669528 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m11:00:35.670214 [info ] [MainThread]:   schema: staging
[0m11:00:35.670800 [info ] [MainThread]:   location: US
[0m11:00:35.671437 [info ] [MainThread]:   priority: None
[0m11:00:35.672033 [info ] [MainThread]:   maximum_bytes_billed: None
[0m11:00:35.672701 [info ] [MainThread]:   impersonate_service_account: None
[0m11:00:35.673415 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m11:00:35.674264 [info ] [MainThread]:   job_retries: 1
[0m11:00:35.674906 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m11:00:35.675568 [info ] [MainThread]:   job_execution_timeout_seconds: None
[0m11:00:35.676163 [info ] [MainThread]:   timeout_seconds: None
[0m11:00:35.676767 [info ] [MainThread]:   client_id: None
[0m11:00:35.677396 [info ] [MainThread]:   token_uri: None
[0m11:00:35.677962 [info ] [MainThread]:   compute_region: None
[0m11:00:35.678618 [info ] [MainThread]:   dataproc_cluster_name: None
[0m11:00:35.679286 [info ] [MainThread]:   gcs_bucket: None
[0m11:00:35.679947 [info ] [MainThread]:   dataproc_batch: None
[0m11:00:35.680920 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:00:35.979337 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m11:00:35.980195 [debug] [MainThread]: On debug: select 1 as id
[0m11:00:35.980877 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:00:35.981745 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: 'Runtime Error
  Failed to authenticate with supplied credentials
  error:
  File credentials/service-account-key.json was not found.'
[0m11:00:35.982428 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m11:00:35.982938 [debug] [MainThread]: BigQuery adapter: Database Error
  Runtime Error
    Failed to authenticate with supplied credentials
    error:
    File credentials/service-account-key.json was not found.
[0m11:00:35.983555 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m11:00:35.984224 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:00:35.984851 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m11:00:35.986331 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 4.0143585, "process_in_blocks": "0", "process_kernel_time": 0.524846, "process_mem_max_rss": "370084", "process_out_blocks": "24", "process_user_time": 7.816715}
[0m11:00:35.987382 [debug] [MainThread]: Command `dbt debug` failed at 11:00:35.987207 after 4.02 seconds
[0m11:00:35.988034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cbe6a350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cb7fe770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7763cb7ffdf0>]}
[0m11:00:35.988689 [debug] [MainThread]: Flushing usage events
[0m11:00:36.224340 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:05:46.650445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbc14f7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbafd7dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbafd7fa0>]}


============================== 11:05:46.655132 | abaadbff-d9af-4b48-b7b1-843d66272e9d ==============================
[0m11:05:46.655132 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:05:46.656144 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'printer_width': '80', 'profiles_dir': '/home/bquser/.dbt', 'write_json': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'version_check': 'True', 'indirect_selection': 'eager', 'empty': 'None', 'no_print': 'None', 'introspect': 'True', 'quiet': 'False', 'invocation_command': 'dbt debug', 'log_cache_events': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_format': 'default'}
[0m11:05:46.668492 [info ] [MainThread]: dbt version: 1.10.11
[0m11:05:46.669342 [info ] [MainThread]: python version: 3.10.12
[0m11:05:46.669963 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m11:05:46.670605 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m11:05:50.047850 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m11:05:50.048804 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m11:05:50.049584 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m11:05:50.050317 [info ] [MainThread]: adapter type: bigquery
[0m11:05:50.050909 [info ] [MainThread]: adapter version: 1.10.2
[0m11:05:50.226926 [info ] [MainThread]: Configuration:
[0m11:05:50.227868 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:05:50.228560 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:05:50.229187 [info ] [MainThread]: Required dependencies:
[0m11:05:50.229799 [debug] [MainThread]: Executing "git --help"
[0m11:05:50.233392 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:05:50.234258 [debug] [MainThread]: STDERR: "b''"
[0m11:05:50.235124 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:05:50.235755 [info ] [MainThread]: Connection:
[0m11:05:50.236446 [info ] [MainThread]:   method: oauth
[0m11:05:50.237189 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m11:05:50.237791 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m11:05:50.238341 [info ] [MainThread]:   schema: staging
[0m11:05:50.239127 [info ] [MainThread]:   location: US
[0m11:05:50.239639 [info ] [MainThread]:   priority: None
[0m11:05:50.240173 [info ] [MainThread]:   maximum_bytes_billed: None
[0m11:05:50.240656 [info ] [MainThread]:   impersonate_service_account: None
[0m11:05:50.241314 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m11:05:50.241906 [info ] [MainThread]:   job_retries: 1
[0m11:05:50.242980 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m11:05:50.243630 [info ] [MainThread]:   job_execution_timeout_seconds: None
[0m11:05:50.244270 [info ] [MainThread]:   timeout_seconds: None
[0m11:05:50.244876 [info ] [MainThread]:   client_id: None
[0m11:05:50.245508 [info ] [MainThread]:   token_uri: None
[0m11:05:50.246129 [info ] [MainThread]:   compute_region: None
[0m11:05:50.246732 [info ] [MainThread]:   dataproc_cluster_name: None
[0m11:05:50.247425 [info ] [MainThread]:   gcs_bucket: None
[0m11:05:50.247993 [info ] [MainThread]:   dataproc_batch: None
[0m11:05:50.248936 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:05:50.545353 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m11:05:50.546161 [debug] [MainThread]: On debug: select 1 as id
[0m11:05:50.546800 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:05:50.547620 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: 'Runtime Error
  Failed to authenticate with supplied credentials
  error:
  File credentials/service-account-key.json was not found.'
[0m11:05:50.548279 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m11:05:50.548759 [debug] [MainThread]: BigQuery adapter: Database Error
  Runtime Error
    Failed to authenticate with supplied credentials
    error:
    File credentials/service-account-key.json was not found.
[0m11:05:50.549288 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m11:05:50.549867 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:05:50.550472 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m11:05:50.551888 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 3.9949377, "process_in_blocks": "0", "process_kernel_time": 0.599001, "process_mem_max_rss": "370088", "process_out_blocks": "24", "process_user_time": 7.76084}
[0m11:05:50.553030 [debug] [MainThread]: Command `dbt debug` failed at 11:05:50.552852 after 4.00 seconds
[0m11:05:50.553695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbc14f7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbbad3070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x703fbbad0220>]}
[0m11:05:50.554497 [debug] [MainThread]: Flushing usage events
[0m11:05:50.761124 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:11:38.993671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de60a2140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de506d750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de506d6f0>]}


============================== 11:11:38.998381 | a2783d71-d385-458e-8575-e9dc99e21c98 ==============================
[0m11:11:38.998381 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:11:38.999391 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'invocation_command': 'dbt debug', 'cache_selected_only': 'False', 'debug': 'False', 'profiles_dir': '/home/bquser/.dbt', 'indirect_selection': 'eager', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'introspect': 'True', 'warn_error': 'None', 'static_parser': 'True', 'write_json': 'True', 'empty': 'None', 'fail_fast': 'False', 'version_check': 'True'}
[0m11:11:39.011172 [info ] [MainThread]: dbt version: 1.10.11
[0m11:11:39.011958 [info ] [MainThread]: python version: 3.10.12
[0m11:11:39.012627 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m11:11:39.013281 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m11:11:42.359611 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m11:11:42.360561 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m11:11:42.361338 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m11:11:42.362102 [info ] [MainThread]: adapter type: bigquery
[0m11:11:42.362712 [info ] [MainThread]: adapter version: 1.10.2
[0m11:11:42.542901 [info ] [MainThread]: Configuration:
[0m11:11:42.543876 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:11:42.544632 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:11:42.545339 [info ] [MainThread]: Required dependencies:
[0m11:11:42.546015 [debug] [MainThread]: Executing "git --help"
[0m11:11:42.549771 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:11:42.550756 [debug] [MainThread]: STDERR: "b''"
[0m11:11:42.551437 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:11:42.552183 [info ] [MainThread]: Connection:
[0m11:11:42.552888 [info ] [MainThread]:   method: service-account
[0m11:11:42.553544 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m11:11:42.554086 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m11:11:42.554664 [info ] [MainThread]:   schema: staging
[0m11:11:42.555268 [info ] [MainThread]:   location: US
[0m11:11:42.555807 [info ] [MainThread]:   priority: None
[0m11:11:42.556374 [info ] [MainThread]:   maximum_bytes_billed: None
[0m11:11:42.556931 [info ] [MainThread]:   impersonate_service_account: None
[0m11:11:42.557527 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m11:11:42.558186 [info ] [MainThread]:   job_retries: 1
[0m11:11:42.558732 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m11:11:42.559373 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m11:11:42.559909 [info ] [MainThread]:   timeout_seconds: 300
[0m11:11:42.560474 [info ] [MainThread]:   client_id: None
[0m11:11:42.561010 [info ] [MainThread]:   token_uri: None
[0m11:11:42.562148 [info ] [MainThread]:   compute_region: None
[0m11:11:42.562800 [info ] [MainThread]:   dataproc_cluster_name: None
[0m11:11:42.563481 [info ] [MainThread]:   gcs_bucket: None
[0m11:11:42.564089 [info ] [MainThread]:   dataproc_batch: None
[0m11:11:42.564940 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:11:42.860034 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m11:11:42.860883 [debug] [MainThread]: On debug: select 1 as id
[0m11:11:42.861616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:11:43.876342 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:d88495b3-c749-4f80-8228-0e7679a04bec&page=queryresults
[0m11:11:44.227275 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:11:44.228290 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:11:44.229823 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 5.330253, "process_in_blocks": "0", "process_kernel_time": 0.533826, "process_mem_max_rss": "379712", "process_out_blocks": "16", "process_user_time": 8.061679}
[0m11:11:44.230721 [debug] [MainThread]: Command `dbt debug` succeeded at 11:11:44.230542 after 5.33 seconds
[0m11:11:44.231323 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:11:44.231975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de60a2140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de59ef4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772de599f940>]}
[0m11:11:44.232689 [debug] [MainThread]: Flushing usage events
[0m11:11:44.457439 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:13:56.254426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8dc5c3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8db366a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8db366a70>]}


============================== 11:13:56.259034 | eb1a4aba-ac99-42c3-a76b-8ddbd3f6e4aa ==============================
[0m11:13:56.259034 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:13:56.260057 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'profiles_dir': '/home/bquser/.dbt', 'use_colors': 'True', 'printer_width': '80', 'target_path': 'None', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'log_format': 'default', 'quiet': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'partial_parse': 'True', 'introspect': 'True', 'write_json': 'True', 'no_print': 'None', 'fail_fast': 'False', 'empty': 'None'}
[0m11:13:56.273526 [info ] [MainThread]: dbt version: 1.10.11
[0m11:13:56.274301 [info ] [MainThread]: python version: 3.10.12
[0m11:13:56.274822 [info ] [MainThread]: python path: /opt/app/bigquery-projects/venv3/bin/python3
[0m11:13:56.275451 [info ] [MainThread]: os info: Linux-6.8.0-1037-gcp-x86_64-with-glibc2.35
[0m11:13:59.667199 [info ] [MainThread]: Using profiles dir at /home/bquser/.dbt
[0m11:13:59.668118 [info ] [MainThread]: Using profiles.yml file at /home/bquser/.dbt/profiles.yml
[0m11:13:59.668842 [info ] [MainThread]: Using dbt_project.yml file at /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/dbt_project.yml
[0m11:13:59.669558 [info ] [MainThread]: adapter type: bigquery
[0m11:13:59.670133 [info ] [MainThread]: adapter version: 1.10.2
[0m11:13:59.845016 [info ] [MainThread]: Configuration:
[0m11:13:59.845824 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:13:59.846463 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:13:59.847117 [info ] [MainThread]: Required dependencies:
[0m11:13:59.847725 [debug] [MainThread]: Executing "git --help"
[0m11:13:59.851258 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:13:59.852085 [debug] [MainThread]: STDERR: "b''"
[0m11:13:59.852830 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:13:59.853527 [info ] [MainThread]: Connection:
[0m11:13:59.854169 [info ] [MainThread]:   method: service-account
[0m11:13:59.854696 [info ] [MainThread]:   database: techshop-data-pipeline-2025
[0m11:13:59.855319 [info ] [MainThread]:   execution_project: techshop-data-pipeline-2025
[0m11:13:59.855861 [info ] [MainThread]:   schema: staging
[0m11:13:59.856444 [info ] [MainThread]:   location: US
[0m11:13:59.856986 [info ] [MainThread]:   priority: None
[0m11:13:59.857570 [info ] [MainThread]:   maximum_bytes_billed: None
[0m11:13:59.858194 [info ] [MainThread]:   impersonate_service_account: None
[0m11:13:59.858868 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m11:13:59.859434 [info ] [MainThread]:   job_retries: 1
[0m11:13:59.860129 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m11:13:59.860732 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m11:13:59.861345 [info ] [MainThread]:   timeout_seconds: 300
[0m11:13:59.861894 [info ] [MainThread]:   client_id: None
[0m11:13:59.862560 [info ] [MainThread]:   token_uri: None
[0m11:13:59.863189 [info ] [MainThread]:   compute_region: None
[0m11:13:59.863857 [info ] [MainThread]:   dataproc_cluster_name: None
[0m11:13:59.864463 [info ] [MainThread]:   gcs_bucket: None
[0m11:13:59.865049 [info ] [MainThread]:   dataproc_batch: None
[0m11:13:59.866106 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:14:00.173355 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m11:14:00.174190 [debug] [MainThread]: On debug: select 1 as id
[0m11:14:00.174868 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:14:00.743140 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:cc16b839-5f26-4911-9792-b25453d3d51e&page=queryresults
[0m11:14:01.055636 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:14:01.056686 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:14:01.058285 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.9014177, "process_in_blocks": "0", "process_kernel_time": 0.539377, "process_mem_max_rss": "379704", "process_out_blocks": "16", "process_user_time": 8.168584}
[0m11:14:01.059223 [debug] [MainThread]: Command `dbt debug` succeeded at 11:14:01.058999 after 4.90 seconds
[0m11:14:01.059810 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:14:01.060744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8dc5c3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8dbc82980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd8dbc82380>]}
[0m11:14:01.061726 [debug] [MainThread]: Flushing usage events
[0m11:14:01.278370 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:19:17.856974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f17afbefb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f179e06f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f179e06fb0>]}


============================== 11:19:17.861652 | 35aea89c-1593-45e9-991f-f5cf4ded9937 ==============================
[0m11:19:17.861652 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:19:17.862724 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'target_path': 'None', 'log_format': 'default', 'debug': 'False', 'profiles_dir': '/home/bquser/.dbt', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'empty': 'None', 'version_check': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt ls', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'static_parser': 'True', 'quiet': 'False', 'fail_fast': 'False'}
[0m11:19:21.502104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f179da7580>]}
[0m11:19:21.609700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f15cd25840>]}
[0m11:19:21.610950 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:19:21.913916 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m11:19:21.915295 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:19:21.916127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f159d8b310>]}
[0m11:19:24.129251 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.techshop_2025.marts
[0m11:19:24.152591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f158908130>]}
[0m11:19:24.323302 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:19:24.326518 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:19:24.345006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f15890ac20>]}
[0m11:19:24.346033 [info ] [MainThread]: Found 3 models, 11 data tests, 5 sources, 495 macros
[0m11:19:24.346824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35aea89c-1593-45e9-991f-f5cf4ded9937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f15890ac80>]}
[0m11:19:24.348542 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m11:19:24.349251 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m11:19:24.349841 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m11:19:24.350453 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m11:19:24.351013 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m11:19:24.351706 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m11:19:24.352398 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m11:19:24.352961 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m11:19:24.353677 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m11:19:24.354329 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m11:19:24.354916 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m11:19:24.355511 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m11:19:24.356110 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m11:19:24.356701 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m11:19:24.357330 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m11:19:24.357906 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m11:19:24.358509 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m11:19:24.359138 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m11:19:24.359706 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m11:19:24.361371 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 6.605827, "process_in_blocks": "56", "process_kernel_time": 0.609826, "process_mem_max_rss": "385476", "process_out_blocks": "2184", "process_user_time": 10.429605}
[0m11:19:24.362344 [debug] [MainThread]: Command `dbt ls` succeeded at 11:19:24.362147 after 6.61 seconds
[0m11:19:24.363024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f17afbefb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f179da7580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78f15895f640>]}
[0m11:19:24.364199 [debug] [MainThread]: Flushing usage events
[0m11:19:24.624682 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:20:08.713613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee29fcb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee28d8bd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee28d8bd90>]}


============================== 11:20:08.717980 | 13349dd7-fd6f-4338-b348-1f57a75a84ea ==============================
[0m11:20:08.717980 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:20:08.719025 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'debug': 'False', 'no_print': 'None', 'target_path': 'None', 'partial_parse': 'True', 'empty': 'False', 'static_parser': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'profiles_dir': '/home/bquser/.dbt', 'use_colors': 'True', 'write_json': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'quiet': 'False'}
[0m11:20:12.241420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee08e6ceb0>]}
[0m11:20:12.349678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee28f6b8b0>]}
[0m11:20:12.350911 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:20:12.657877 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m11:20:12.886022 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:20:12.886789 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:20:12.897862 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.techshop_2025.marts
[0m11:20:12.974680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee08510130>]}
[0m11:20:13.143361 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:20:13.146394 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:20:13.164927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee085110c0>]}
[0m11:20:13.165988 [info ] [MainThread]: Found 3 models, 11 data tests, 5 sources, 495 macros
[0m11:20:13.166733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee08510b20>]}
[0m11:20:13.169797 [info ] [MainThread]: 
[0m11:20:13.170565 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:20:13.171201 [info ] [MainThread]: 
[0m11:20:13.172160 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:20:13.179974 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m11:20:13.181170 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:13.568206 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now create_techshop-data-pipeline-2025_staging_staging)
[0m11:20:13.569526 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "staging_staging"
"
[0m11:20:13.588634 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_staging_staging: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_staging_staging"} */
create schema if not exists `techshop-data-pipeline-2025`.`staging_staging`
  
[0m11:20:13.589593 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:20:14.107465 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:79dab144-ee6d-43d7-ae37-31676d46c190&page=queryresults
[0m11:20:15.016788 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_techshop-data-pipeline-2025_staging_staging, now list_techshop-data-pipeline-2025_staging_staging)
[0m11:20:15.018032 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:20:15.396674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee08fe9f00>]}
[0m11:20:15.397916 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:20:15.453536 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m11:20:15.454697 [info ] [Thread-1 (]: 1 of 3 START sql view model staging_staging.stg_customers ...................... [RUN]
[0m11:20:15.456318 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_staging, now model.techshop_2025.stg_customers)
[0m11:20:15.457123 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m11:20:15.471532 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m11:20:15.472926 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m11:20:15.510600 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m11:20:15.512266 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`staging_staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m11:20:15.513280 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:16.087306 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:f0ed427c-98c5-4efc-851f-f5954c19bfc3&page=queryresults
[0m11:20:16.598169 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee0855c910>]}
[0m11:20:16.599573 [info ] [Thread-1 (]: 1 of 3 OK created sql view model staging_staging.stg_customers ................. [[32mCREATE VIEW (0 processed)[0m in 1.14s]
[0m11:20:16.600798 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m11:20:16.601567 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m11:20:16.602608 [info ] [Thread-1 (]: 2 of 3 START sql view model staging_staging.stg_orders ......................... [RUN]
[0m11:20:16.603554 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m11:20:16.604246 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m11:20:16.612807 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m11:20:16.613923 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m11:20:16.619181 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m11:20:16.620435 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`staging_staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m11:20:16.621320 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:17.268720 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:aac24ac6-a889-43c9-b88b-f9781edbc114&page=queryresults
[0m11:20:17.650029 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee2a03fdf0>]}
[0m11:20:17.651380 [info ] [Thread-1 (]: 2 of 3 OK created sql view model staging_staging.stg_orders .................... [[32mCREATE VIEW (0 processed)[0m in 1.05s]
[0m11:20:17.652603 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m11:20:17.653433 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m11:20:17.654495 [info ] [Thread-1 (]: 3 of 3 START sql view model staging_staging.stg_products ....................... [RUN]
[0m11:20:17.655483 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m11:20:17.656168 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m11:20:17.662783 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m11:20:17.663874 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m11:20:17.669311 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m11:20:17.670519 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`staging_staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m11:20:17.671383 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:18.269239 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:c08a50e1-627c-4003-8bce-47dd67a09377&page=queryresults
[0m11:20:18.620375 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13349dd7-fd6f-4338-b348-1f57a75a84ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee078d3a60>]}
[0m11:20:18.621749 [info ] [Thread-1 (]: 3 of 3 OK created sql view model staging_staging.stg_products .................. [[32mCREATE VIEW (0 processed)[0m in 0.96s]
[0m11:20:18.623017 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m11:20:18.625044 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:20:18.674832 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:20:18.675738 [debug] [MainThread]: Connection 'model.techshop_2025.stg_products' was properly closed.
[0m11:20:18.676593 [info ] [MainThread]: 
[0m11:20:18.677353 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 5.50 seconds (5.50s).
[0m11:20:18.679027 [debug] [MainThread]: Command end result
[0m11:20:18.723676 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:20:18.726554 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:20:18.737142 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m11:20:18.737830 [info ] [MainThread]: 
[0m11:20:18.738698 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:20:18.739386 [info ] [MainThread]: 
[0m11:20:18.740104 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m11:20:18.741700 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.123319, "process_in_blocks": "16", "process_kernel_time": 0.618133, "process_mem_max_rss": "389060", "process_out_blocks": "2288", "process_user_time": 9.888132}
[0m11:20:18.742533 [debug] [MainThread]: Command `dbt run` succeeded at 11:20:18.742357 after 10.12 seconds
[0m11:20:18.743224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee29fcb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee28f6b8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cee08e6c5e0>]}
[0m11:20:18.743948 [debug] [MainThread]: Flushing usage events
[0m11:20:18.939649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:20:47.565224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e70e6b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e5e73ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e5e73a60>]}


============================== 11:20:47.569777 | 6d655502-4245-415a-b2ee-473d126ef863 ==============================
[0m11:20:47.569777 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:20:47.570760 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'log_cache_events': 'False', 'log_format': 'default', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'profiles_dir': '/home/bquser/.dbt', 'target_path': 'None', 'empty': 'None', 'invocation_command': 'dbt test', 'use_colors': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'printer_width': '80', 'fail_fast': 'False'}
[0m11:20:51.164443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303c8edb1f0>]}
[0m11:20:51.269984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e6559570>]}
[0m11:20:51.271238 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:20:51.579978 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m11:20:51.816850 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:20:51.817649 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:20:51.829653 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.techshop_2025.marts
[0m11:20:51.909600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303c560e710>]}
[0m11:20:52.076422 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:20:52.079381 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:20:52.113978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303c56194b0>]}
[0m11:20:52.115137 [info ] [MainThread]: Found 3 models, 11 data tests, 5 sources, 495 macros
[0m11:20:52.115937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303c560dde0>]}
[0m11:20:52.119430 [info ] [MainThread]: 
[0m11:20:52.120341 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:20:52.121083 [info ] [MainThread]: 
[0m11:20:52.122248 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:20:52.131233 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025_staging_staging'
[0m11:20:52.132054 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:52.500506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d655502-4245-415a-b2ee-473d126ef863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e713f6d0>]}
[0m11:20:52.501630 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:20:52.555501 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m11:20:52.556455 [info ] [Thread-1 (]: 1 of 11 START test source_not_null_raw_data_customers_customer_id .............. [RUN]
[0m11:20:52.557524 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_staging, now test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b)
[0m11:20:52.558277 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m11:20:52.588679 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b"
[0m11:20:52.589986 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m11:20:52.623928 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b"
[0m11:20:52.625326 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `techshop-data-pipeline-2025`.`raw_data`.`customers`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m11:20:52.626219 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:54.644182 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:94f5099e-8107-4d26-af9c-d979cb4a0d21&page=queryresults
[0m11:20:55.247864 [info ] [Thread-1 (]: 1 of 11 PASS source_not_null_raw_data_customers_customer_id .................... [[32mPASS[0m in 2.69s]
[0m11:20:55.249461 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m11:20:55.250353 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m11:20:55.251275 [info ] [Thread-1 (]: 2 of 11 START test source_not_null_raw_data_marketing_campaigns_campaign_id .... [RUN]
[0m11:20:55.252626 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b, now test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c)
[0m11:20:55.253649 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m11:20:55.261888 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c"
[0m11:20:55.263110 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m11:20:55.267914 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c"
[0m11:20:55.269238 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select campaign_id
from `techshop-data-pipeline-2025`.`raw_data`.`marketing_campaigns`
where campaign_id is null



  
  
      
    ) dbt_internal_test
[0m11:20:55.270137 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:56.496292 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:0dce8a41-8e6d-46a2-ae15-18dbbbce153d&page=queryresults
[0m11:20:56.912941 [info ] [Thread-1 (]: 2 of 11 PASS source_not_null_raw_data_marketing_campaigns_campaign_id .......... [[32mPASS[0m in 1.66s]
[0m11:20:56.914329 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m11:20:56.915089 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m11:20:56.915907 [info ] [Thread-1 (]: 3 of 11 START test source_not_null_raw_data_order_items_order_id ............... [RUN]
[0m11:20:56.916904 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c, now test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406)
[0m11:20:56.917592 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m11:20:56.925442 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406"
[0m11:20:56.926502 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m11:20:56.931197 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406"
[0m11:20:56.932281 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `techshop-data-pipeline-2025`.`raw_data`.`order_items`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m11:20:56.933008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:57.626862 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:b381f0df-6bbe-461b-8cf1-1d276dae79f9&page=queryresults
[0m11:20:58.229083 [info ] [Thread-1 (]: 3 of 11 PASS source_not_null_raw_data_order_items_order_id ..................... [[32mPASS[0m in 1.31s]
[0m11:20:58.230532 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m11:20:58.231335 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m11:20:58.232514 [info ] [Thread-1 (]: 4 of 11 START test source_not_null_raw_data_order_items_product_id ............. [RUN]
[0m11:20:58.233620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406, now test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7)
[0m11:20:58.234492 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m11:20:58.242354 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7"
[0m11:20:58.243476 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m11:20:58.249979 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7"
[0m11:20:58.251130 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from `techshop-data-pipeline-2025`.`raw_data`.`order_items`
where product_id is null



  
  
      
    ) dbt_internal_test
[0m11:20:58.251868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:20:59.169303 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:172d97a7-50b1-4fc4-9dce-ecdd16452a59&page=queryresults
[0m11:20:59.575161 [info ] [Thread-1 (]: 4 of 11 PASS source_not_null_raw_data_order_items_product_id ................... [[32mPASS[0m in 1.34s]
[0m11:20:59.576515 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m11:20:59.577293 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m11:20:59.578291 [info ] [Thread-1 (]: 5 of 11 START test source_not_null_raw_data_orders_customer_id ................. [RUN]
[0m11:20:59.579246 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7, now test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9)
[0m11:20:59.579988 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m11:20:59.587690 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9"
[0m11:20:59.588720 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m11:20:59.593250 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9"
[0m11:20:59.594297 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `techshop-data-pipeline-2025`.`raw_data`.`orders`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m11:20:59.595007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:00.756149 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:a59808b3-d946-42ce-98f3-844977f5e95f&page=queryresults
[0m11:21:01.128617 [info ] [Thread-1 (]: 5 of 11 PASS source_not_null_raw_data_orders_customer_id ....................... [[32mPASS[0m in 1.55s]
[0m11:21:01.130054 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m11:21:01.130897 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m11:21:01.131856 [info ] [Thread-1 (]: 6 of 11 START test source_not_null_raw_data_orders_order_id .................... [RUN]
[0m11:21:01.132931 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9, now test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8)
[0m11:21:01.133712 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m11:21:01.141805 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8"
[0m11:21:01.142992 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m11:21:01.147837 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8"
[0m11:21:01.149028 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `techshop-data-pipeline-2025`.`raw_data`.`orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m11:21:01.149860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:02.253004 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:9509905f-93c8-47d7-9a96-4101465737be&page=queryresults
[0m11:21:02.669014 [info ] [Thread-1 (]: 6 of 11 PASS source_not_null_raw_data_orders_order_id .......................... [[32mPASS[0m in 1.53s]
[0m11:21:02.670539 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m11:21:02.671403 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m11:21:02.672291 [info ] [Thread-1 (]: 7 of 11 START test source_not_null_raw_data_products_product_id ................ [RUN]
[0m11:21:02.673291 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8, now test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4)
[0m11:21:02.674014 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m11:21:02.682300 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4"
[0m11:21:02.683601 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m11:21:02.690317 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4"
[0m11:21:02.691533 [debug] [Thread-1 (]: On test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from `techshop-data-pipeline-2025`.`raw_data`.`products`
where product_id is null



  
  
      
    ) dbt_internal_test
[0m11:21:02.692406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:03.953513 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:0d2620f2-82f3-46b9-934e-d95468fa079c&page=queryresults
[0m11:21:04.355048 [info ] [Thread-1 (]: 7 of 11 PASS source_not_null_raw_data_products_product_id ...................... [[32mPASS[0m in 1.68s]
[0m11:21:04.356504 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m11:21:04.357362 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m11:21:04.358182 [info ] [Thread-1 (]: 8 of 11 START test source_unique_raw_data_customers_customer_id ................ [RUN]
[0m11:21:04.359409 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4, now test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856)
[0m11:21:04.360220 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m11:21:04.371885 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856"
[0m11:21:04.373132 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m11:21:04.378106 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856"
[0m11:21:04.379357 [debug] [Thread-1 (]: On test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `techshop-data-pipeline-2025`.`raw_data`.`customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:21:04.380204 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:06.467048 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:1519dc5e-e960-45df-a41f-9fc0ad94fc1a&page=queryresults
[0m11:21:06.969816 [info ] [Thread-1 (]: 8 of 11 PASS source_unique_raw_data_customers_customer_id ...................... [[32mPASS[0m in 2.61s]
[0m11:21:06.971177 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m11:21:06.971994 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m11:21:06.972884 [info ] [Thread-1 (]: 9 of 11 START test source_unique_raw_data_marketing_campaigns_campaign_id ...... [RUN]
[0m11:21:06.973902 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856, now test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b)
[0m11:21:06.974644 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m11:21:06.982623 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b"
[0m11:21:06.983764 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m11:21:06.988700 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b"
[0m11:21:06.990447 [debug] [Thread-1 (]: On test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select campaign_id as unique_field
  from `techshop-data-pipeline-2025`.`raw_data`.`marketing_campaigns`
  where campaign_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:21:06.991332 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:07.944948 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:10021508-edbb-41b7-bbd8-04c0b26631fe&page=queryresults
[0m11:21:08.287286 [info ] [Thread-1 (]: 9 of 11 PASS source_unique_raw_data_marketing_campaigns_campaign_id ............ [[32mPASS[0m in 1.31s]
[0m11:21:08.288668 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m11:21:08.289519 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m11:21:08.290370 [info ] [Thread-1 (]: 10 of 11 START test source_unique_raw_data_orders_order_id ..................... [RUN]
[0m11:21:08.291364 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b, now test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8)
[0m11:21:08.292121 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m11:21:08.300371 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8"
[0m11:21:08.301499 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m11:21:08.308314 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8"
[0m11:21:08.309675 [debug] [Thread-1 (]: On test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `techshop-data-pipeline-2025`.`raw_data`.`orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:21:08.310567 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:09.615812 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:a9c544b2-a010-4eb0-a082-ecf6b510d53f&page=queryresults
[0m11:21:09.984258 [info ] [Thread-1 (]: 10 of 11 PASS source_unique_raw_data_orders_order_id ........................... [[32mPASS[0m in 1.69s]
[0m11:21:09.985768 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m11:21:09.986633 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m11:21:09.987597 [info ] [Thread-1 (]: 11 of 11 START test source_unique_raw_data_products_product_id ................. [RUN]
[0m11:21:09.988743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8, now test.techshop_2025.source_unique_raw_data_products_product_id.6029824217)
[0m11:21:09.989534 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m11:21:09.998019 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_products_product_id.6029824217"
[0m11:21:09.999224 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m11:21:10.004529 [debug] [Thread-1 (]: Writing runtime sql for node "test.techshop_2025.source_unique_raw_data_products_product_id.6029824217"
[0m11:21:10.005775 [debug] [Thread-1 (]: On test.techshop_2025.source_unique_raw_data_products_product_id.6029824217: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "test.techshop_2025.source_unique_raw_data_products_product_id.6029824217"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select product_id as unique_field
  from `techshop-data-pipeline-2025`.`raw_data`.`products`
  where product_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:21:10.006692 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:11.483922 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:edceecfe-cbc7-498e-a839-883a8d0c6f78&page=queryresults
[0m11:21:11.847493 [info ] [Thread-1 (]: 11 of 11 PASS source_unique_raw_data_products_product_id ....................... [[32mPASS[0m in 1.86s]
[0m11:21:11.848993 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m11:21:11.851486 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:21:11.901427 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:21:11.902200 [debug] [MainThread]: Connection 'test.techshop_2025.source_unique_raw_data_products_product_id.6029824217' was properly closed.
[0m11:21:11.902968 [info ] [MainThread]: 
[0m11:21:11.903717 [info ] [MainThread]: Finished running 11 data tests in 0 hours 0 minutes and 19.78 seconds (19.78s).
[0m11:21:11.907238 [debug] [MainThread]: Command end result
[0m11:21:11.951533 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:21:11.954609 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:21:11.967098 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m11:21:11.967868 [info ] [MainThread]: 
[0m11:21:11.968651 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:21:11.969311 [info ] [MainThread]: 
[0m11:21:11.969920 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=11
[0m11:21:11.971555 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 24.499838, "process_in_blocks": "0", "process_kernel_time": 0.593302, "process_mem_max_rss": "389620", "process_out_blocks": "2456", "process_user_time": 11.398981}
[0m11:21:11.972358 [debug] [MainThread]: Command `dbt test` succeeded at 11:21:11.972182 after 24.50 seconds
[0m11:21:11.972990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e70e6b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303c8edb1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7303e6d6e710>]}
[0m11:21:11.973703 [debug] [MainThread]: Flushing usage events
[0m11:21:12.189466 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:48:39.932048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebadfaafd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebade7f0730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebade7f1fc0>]}


============================== 11:48:39.936567 | 390a5801-a557-44db-a4bb-955a9c9c1c86 ==============================
[0m11:48:39.936567 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:48:39.937608 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'target_path': 'None', 'no_print': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'empty': 'None', 'write_json': 'True', 'static_parser': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt ls', 'warn_error': 'None', 'profiles_dir': '/home/bquser/.dbt'}
[0m11:48:43.590631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebabe9e7d90>]}
[0m11:48:43.706225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebac160f400>]}
[0m11:48:43.707564 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:48:44.012686 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m11:48:44.201769 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:48:44.202777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebabe9e75b0>]}
[0m11:48:46.434584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebabd351900>]}
[0m11:48:46.602709 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:48:46.606349 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:48:46.624321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebabd304b50>]}
[0m11:48:46.625351 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m11:48:46.626045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '390a5801-a557-44db-a4bb-955a9c9c1c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebabd304be0>]}
[0m11:48:46.627673 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m11:48:46.628351 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m11:48:46.628877 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m11:48:46.629450 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m11:48:46.629980 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m11:48:46.630577 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m11:48:46.631191 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m11:48:46.631746 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m11:48:46.632516 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m11:48:46.633250 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m11:48:46.633806 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m11:48:46.634427 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m11:48:46.635021 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m11:48:46.635669 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m11:48:46.636278 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m11:48:46.636974 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m11:48:46.637781 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m11:48:46.638598 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m11:48:46.639393 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m11:48:46.640165 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m11:48:46.642254 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 6.8047633, "process_in_blocks": "0", "process_kernel_time": 0.566456, "process_mem_max_rss": "385636", "process_out_blocks": "2200", "process_user_time": 10.570693}
[0m11:48:46.643406 [debug] [MainThread]: Command `dbt ls` succeeded at 11:48:46.643214 after 6.81 seconds
[0m11:48:46.644055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebadfaafd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebac160f400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ebae060a560>]}
[0m11:48:46.644835 [debug] [MainThread]: Flushing usage events
[0m11:48:46.875645 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:49:20.676954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f5455f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f533caf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f533caf80>]}


============================== 11:49:20.681386 | 953c49cb-6fd9-46fe-9dae-a3a886ca0549 ==============================
[0m11:49:20.681386 [info ] [MainThread]: Running with dbt=1.10.11
[0m11:49:20.682464 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'target_path': 'None', 'static_parser': 'True', 'profiles_dir': '/home/bquser/.dbt', 'printer_width': '80', 'indirect_selection': 'eager', 'empty': 'False', 'debug': 'False', 'quiet': 'False', 'warn_error': 'None', 'introspect': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'version_check': 'True', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'write_json': 'True', 'no_print': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False'}
[0m11:49:24.241696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f336708b0>]}
[0m11:49:24.349224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f535bbc10>]}
[0m11:49:24.350479 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:49:24.667418 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m11:49:24.892325 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:49:24.893075 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:49:24.980273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f53ea0130>]}
[0m11:49:25.151182 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:49:25.154559 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:49:25.174640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f53ed0fd0>]}
[0m11:49:25.175779 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m11:49:25.176540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f53ea3640>]}
[0m11:49:25.179578 [info ] [MainThread]: 
[0m11:49:25.180355 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:49:25.180978 [info ] [MainThread]: 
[0m11:49:25.182048 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:49:25.190476 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m11:49:25.191871 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:25.557310 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:49:25.903924 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now create_techshop-data-pipeline-2025_staging_marts)
[0m11:49:25.905131 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "staging_marts"
"
[0m11:49:25.926776 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_staging_marts: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_staging_marts"} */
create schema if not exists `techshop-data-pipeline-2025`.`staging_marts`
  
[0m11:49:25.927781 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:49:26.574840 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:28b8c8fa-1f65-4fbd-8183-276903b79025&page=queryresults
[0m11:49:27.455809 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_techshop-data-pipeline-2025_staging_marts, now list_techshop-data-pipeline-2025_staging)
[0m11:49:27.456855 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:49:27.803688 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging, now list_techshop-data-pipeline-2025_staging_marts)
[0m11:49:27.804721 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:49:28.140354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f55fba710>]}
[0m11:49:28.141559 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:49:28.198544 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m11:49:28.199872 [info ] [Thread-1 (]: 1 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m11:49:28.200986 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_marts, now model.techshop_2025.stg_customers)
[0m11:49:28.201768 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m11:49:28.215558 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m11:49:28.216756 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m11:49:28.254544 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m11:49:28.255919 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m11:49:28.257157 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:28.964673 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:a1fcd3cf-a3c8-4d87-8a12-a7b5a949592b&page=queryresults
[0m11:49:29.326622 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f335eb880>]}
[0m11:49:29.328148 [info ] [Thread-1 (]: 1 of 4 OK created sql view model staging.stg_customers ......................... [[32mCREATE VIEW (0 processed)[0m in 1.12s]
[0m11:49:29.329671 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m11:49:29.330752 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m11:49:29.331977 [info ] [Thread-1 (]: 2 of 4 START sql view model staging.stg_orders ................................. [RUN]
[0m11:49:29.333080 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m11:49:29.333805 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m11:49:29.340745 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m11:49:29.341995 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m11:49:29.348087 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m11:49:29.349440 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m11:49:29.350472 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:29.911555 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:0a637e35-2839-4650-8ef1-37955b7d976a&page=queryresults
[0m11:49:30.258786 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f544f7070>]}
[0m11:49:30.260174 [info ] [Thread-1 (]: 2 of 4 OK created sql view model staging.stg_orders ............................ [[32mCREATE VIEW (0 processed)[0m in 0.93s]
[0m11:49:30.261401 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m11:49:30.262207 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m11:49:30.263251 [info ] [Thread-1 (]: 3 of 4 START sql view model staging.stg_products ............................... [RUN]
[0m11:49:30.264167 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m11:49:30.264809 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m11:49:30.271026 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m11:49:30.272159 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m11:49:30.277433 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m11:49:30.278751 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m11:49:30.279764 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:30.813589 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:3d9bf296-dc86-42de-8f69-588f058a5c6e&page=queryresults
[0m11:49:31.106603 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f544f7070>]}
[0m11:49:31.108148 [info ] [Thread-1 (]: 3 of 4 OK created sql view model staging.stg_products .......................... [[32mCREATE VIEW (0 processed)[0m in 0.84s]
[0m11:49:31.109425 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m11:49:31.110927 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m11:49:31.112292 [info ] [Thread-1 (]: 4 of 4 START sql table model staging_marts.fct_sales ........................... [RUN]
[0m11:49:31.113375 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now model.techshop_2025.fct_sales)
[0m11:49:31.114427 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m11:49:31.122274 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m11:49:31.123446 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m11:49:31.171678 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m11:49:31.173046 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`staging_marts`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      # Vérifier la structure des dossiers
ls -la models/
ls -la models/staging/
ls -la models/marts/

# Créer le fichier fct_sales s'il manque
cat > models/marts/fct_sales.sql << 'EOF'


SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
EOF
    );
  
[0m11:49:31.174031 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:31.489749 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:edcd0bce-cef6-4366-be11-40c9ba6c99d3&page=queryresults
[0m11:49:31.490971 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:edcd0bce-cef6-4366-be11-40c9ba6c99d3&page=queryresults
[0m11:49:31.499488 [debug] [Thread-1 (]: Database Error in model fct_sales (models/marts/fct_sales.sql)
  Syntax error: Unexpected identifier "ls" at [15:1]
  compiled code at target/run/techshop_2025/models/marts/fct_sales.sql
[0m11:49:31.500720 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '953c49cb-6fd9-46fe-9dae-a3a886ca0549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f307264a0>]}
[0m11:49:31.502076 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model staging_marts.fct_sales .................. [[31mERROR[0m in 0.39s]
[0m11:49:31.503176 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m11:49:31.504309 [debug] [Thread-4 (]: Marking all children of 'model.techshop_2025.fct_sales' to be skipped because of status 'error'.  Reason: Database Error in model fct_sales (models/marts/fct_sales.sql)
  Syntax error: Unexpected identifier "ls" at [15:1]
  compiled code at target/run/techshop_2025/models/marts/fct_sales.sql.
[0m11:49:31.507287 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:49:31.557137 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:31.557929 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m11:49:31.558868 [info ] [MainThread]: 
[0m11:49:31.559631 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 6.38 seconds (6.38s).
[0m11:49:31.561564 [debug] [MainThread]: Command end result
[0m11:49:31.612280 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m11:49:31.615862 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m11:49:31.627027 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m11:49:31.628020 [info ] [MainThread]: 
[0m11:49:31.628811 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:49:31.629602 [info ] [MainThread]: 
[0m11:49:31.630434 [error] [MainThread]: [31mFailure in model fct_sales (models/marts/fct_sales.sql)[0m
[0m11:49:31.631226 [error] [MainThread]:   Database Error in model fct_sales (models/marts/fct_sales.sql)
  Syntax error: Unexpected identifier "ls" at [15:1]
  compiled code at target/run/techshop_2025/models/marts/fct_sales.sql
[0m11:49:31.631919 [info ] [MainThread]: 
[0m11:49:31.632701 [info ] [MainThread]:   compiled code at target/compiled/techshop_2025/models/marts/fct_sales.sql
[0m11:49:31.633347 [info ] [MainThread]: 
[0m11:49:31.634019 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m11:49:31.635725 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 11.05578, "process_in_blocks": "0", "process_kernel_time": 0.630443, "process_mem_max_rss": "389968", "process_out_blocks": "2352", "process_user_time": 10.5872}
[0m11:49:31.636592 [debug] [MainThread]: Command `dbt run` failed at 11:49:31.636412 after 11.06 seconds
[0m11:49:31.637335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f5455f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f336708b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f53f0f4f0>]}
[0m11:49:31.638097 [debug] [MainThread]: Flushing usage events
[0m11:49:31.854657 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:02:54.823353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cf8ea4040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cf8d81de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cf7df2bc0>]}


============================== 12:02:54.828639 | 11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2 ==============================
[0m12:02:54.828639 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:02:54.830133 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/bquser/.dbt', 'no_print': 'None', 'use_colors': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'empty': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt ls', 'use_experimental_parser': 'False', 'static_parser': 'True', 'write_json': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'cache_selected_only': 'False', 'introspect': 'True'}
[0m12:02:58.573408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd80c9600>]}
[0m12:02:58.685235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd7f28790>]}
[0m12:02:58.686581 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:02:59.006964 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:02:59.214722 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:02:59.215828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cf84f1810>]}
[0m12:03:01.464196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd6c4e5c0>]}
[0m12:03:01.633636 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:03:01.638627 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:03:01.660170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd6c04130>]}
[0m12:03:01.661325 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m12:03:01.662161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11a4dfa0-1c0b-4cbd-8de2-d63d0a72dab2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd7589060>]}
[0m12:03:01.663931 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m12:03:01.664657 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m12:03:01.665294 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m12:03:01.665863 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m12:03:01.666442 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m12:03:01.667003 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m12:03:01.667565 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m12:03:01.668326 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m12:03:01.668872 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m12:03:01.669621 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m12:03:01.670261 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m12:03:01.670845 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m12:03:01.671708 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m12:03:01.672364 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m12:03:01.673127 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m12:03:01.673714 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m12:03:01.674352 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m12:03:01.674938 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m12:03:01.675649 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m12:03:01.676271 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m12:03:01.677745 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 6.953348, "process_in_blocks": "0", "process_kernel_time": 0.60524, "process_mem_max_rss": "385776", "process_out_blocks": "2200", "process_user_time": 10.971628}
[0m12:03:01.678597 [debug] [MainThread]: Command `dbt ls` succeeded at 12:03:01.678425 after 6.95 seconds
[0m12:03:01.679281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cf8ea4040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd7589060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d4cd7f28790>]}
[0m12:03:01.679994 [debug] [MainThread]: Flushing usage events
[0m12:03:01.899598 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:04:23.452132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2ec52170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2dc04520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2dc044c0>]}


============================== 12:04:23.456657 | 170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9 ==============================
[0m12:04:23.456657 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:04:23.457753 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'printer_width': '80', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'cache_selected_only': 'False', 'fail_fast': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'profiles_dir': '/home/bquser/.dbt', 'write_json': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'empty': 'False'}
[0m12:04:26.985012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f303709a0>]}
[0m12:04:27.092185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2eb04790>]}
[0m12:04:27.093434 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:04:27.396151 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:04:27.618793 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:04:27.619608 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:04:27.710439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e642530>]}
[0m12:04:27.878857 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:04:27.881846 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:04:27.900588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e62b640>]}
[0m12:04:27.901610 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m12:04:27.902371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e7f3130>]}
[0m12:04:27.905251 [info ] [MainThread]: 
[0m12:04:27.906016 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:04:27.906672 [info ] [MainThread]: 
[0m12:04:27.907658 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:04:27.915703 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m12:04:27.916821 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:04:28.317047 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now list_techshop-data-pipeline-2025_staging)
[0m12:04:28.318238 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:04:28.648889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2dd5e770>]}
[0m12:04:28.650177 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:04:28.706575 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m12:04:28.707739 [info ] [Thread-1 (]: 1 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m12:04:28.708923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging, now model.techshop_2025.stg_customers)
[0m12:04:28.709768 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m12:04:28.727122 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m12:04:28.728661 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m12:04:28.774729 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m12:04:28.776117 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m12:04:28.777041 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:29.422798 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:c84fddc8-ec17-4cbe-afa3-d2ae686cbe06&page=queryresults
[0m12:04:29.844229 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f10b224d0>]}
[0m12:04:29.845621 [info ] [Thread-1 (]: 1 of 4 OK created sql view model staging.stg_customers ......................... [[32mCREATE VIEW (0 processed)[0m in 1.13s]
[0m12:04:29.846773 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m12:04:29.847578 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m12:04:29.848648 [info ] [Thread-1 (]: 2 of 4 START sql view model staging.stg_orders ................................. [RUN]
[0m12:04:29.849624 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m12:04:29.850319 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m12:04:29.857041 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m12:04:29.858200 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m12:04:29.864847 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m12:04:29.866165 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m12:04:29.867052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:30.370798 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:4d69ddba-a122-4005-97c7-f790f60b69fb&page=queryresults
[0m12:04:30.762223 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e6965f0>]}
[0m12:04:30.763612 [info ] [Thread-1 (]: 2 of 4 OK created sql view model staging.stg_orders ............................ [[32mCREATE VIEW (0 processed)[0m in 0.91s]
[0m12:04:30.764850 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m12:04:30.765655 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m12:04:30.766768 [info ] [Thread-1 (]: 3 of 4 START sql view model staging.stg_products ............................... [RUN]
[0m12:04:30.767861 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m12:04:30.768769 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m12:04:30.775853 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m12:04:30.777087 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m12:04:30.782331 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m12:04:30.783626 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m12:04:30.784544 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:31.411467 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:d91c76ca-cae8-43e3-87cb-9e72bb2a63b1&page=queryresults
[0m12:04:31.769968 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e641c30>]}
[0m12:04:31.771465 [info ] [Thread-1 (]: 3 of 4 OK created sql view model staging.stg_products .......................... [[32mCREATE VIEW (0 processed)[0m in 1.00s]
[0m12:04:31.772755 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m12:04:31.774401 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m12:04:31.775714 [info ] [Thread-1 (]: 4 of 4 START sql table model staging.fct_sales ................................. [RUN]
[0m12:04:31.777461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now model.techshop_2025.fct_sales)
[0m12:04:31.778408 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m12:04:31.786421 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m12:04:31.787713 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m12:04:31.836267 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m12:04:31.837737 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`staging`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      -- dbt_project/models/marts/fct_sales.sql


SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
    );
  
[0m12:04:31.838845 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:32.754006 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:d230384b-2384-4a8f-8eba-b3d29e069daa&page=queryresults
[0m12:04:39.396830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '170e867f-ca1d-4bfc-9a05-a8b8fdcc89f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f06c107f0>]}
[0m12:04:39.398435 [info ] [Thread-1 (]: 4 of 4 OK created sql table model staging.fct_sales ............................ [[32mCREATE TABLE (189.7k rows, 19.2 MiB processed)[0m in 7.62s]
[0m12:04:39.399805 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m12:04:39.402563 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:04:39.452332 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:04:39.453158 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m12:04:39.453971 [info ] [MainThread]: 
[0m12:04:39.454693 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 11.55 seconds (11.55s).
[0m12:04:39.456507 [debug] [MainThread]: Command end result
[0m12:04:39.497223 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:04:39.500342 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:04:39.511852 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m12:04:39.512664 [info ] [MainThread]: 
[0m12:04:39.513561 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:04:39.514284 [info ] [MainThread]: 
[0m12:04:39.515004 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m12:04:39.516645 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.164797, "process_in_blocks": "0", "process_kernel_time": 0.59059, "process_mem_max_rss": "389256", "process_out_blocks": "2352", "process_user_time": 10.098924}
[0m12:04:39.517530 [debug] [MainThread]: Command `dbt run` succeeded at 12:04:39.517348 after 16.17 seconds
[0m12:04:39.518231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2ec52170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f303709a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f2e7fb130>]}
[0m12:04:39.518992 [debug] [MainThread]: Flushing usage events
[0m12:04:39.790776 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:22:59.372670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e5bc3460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e4a2fd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e4a2fdf0>]}


============================== 12:22:59.377733 | 93781c4a-2e86-47de-8a11-39cbac87df91 ==============================
[0m12:22:59.377733 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:22:59.378859 [debug] [MainThread]: running dbt with arguments {'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'empty': 'None', 'profiles_dir': '/home/bquser/.dbt', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'no_print': 'None', 'version_check': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'fail_fast': 'False', 'log_format': 'default', 'invocation_command': 'dbt ls', 'target_path': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'quiet': 'False'}
[0m12:23:03.849181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c4a52770>]}
[0m12:23:03.992369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e5135690>]}
[0m12:23:03.993760 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:23:04.396648 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:23:04.699754 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:23:04.701164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e50dd9c0>]}
[0m12:23:07.782555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c38eda50>]}
[0m12:23:07.990161 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:23:07.993840 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:23:08.016530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c37145e0>]}
[0m12:23:08.018333 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m12:23:08.019357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93781c4a-2e86-47de-8a11-39cbac87df91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c3715cc0>]}
[0m12:23:08.021703 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m12:23:08.022791 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m12:23:08.023755 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m12:23:08.024526 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m12:23:08.025811 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m12:23:08.027532 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m12:23:08.028598 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m12:23:08.029310 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m12:23:08.030302 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m12:23:08.031466 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m12:23:08.032677 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m12:23:08.033890 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m12:23:08.035166 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m12:23:08.035853 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m12:23:08.036688 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m12:23:08.037391 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m12:23:08.038114 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m12:23:08.038761 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m12:23:08.039388 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m12:23:08.039965 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m12:23:08.041595 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 8.770026, "process_in_blocks": "0", "process_kernel_time": 0.818126, "process_mem_max_rss": "385736", "process_out_blocks": "2208", "process_user_time": 13.226122}
[0m12:23:08.042993 [debug] [MainThread]: Command `dbt ls` succeeded at 12:23:08.042724 after 8.77 seconds
[0m12:23:08.044469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4e5bc3460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c4a52770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79d4c38ecc10>]}
[0m12:23:08.045946 [debug] [MainThread]: Flushing usage events
[0m12:23:08.319179 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:25:40.674028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3fdb5c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3ed783d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3ed78370>]}


============================== 12:25:40.678704 | 8a594d27-1348-4c14-bea3-563dc7d80c6b ==============================
[0m12:25:40.678704 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:25:40.679788 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/home/bquser/.dbt', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'log_format': 'default', 'debug': 'False', 'invocation_command': 'dbt run', 'use_colors': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'version_check': 'True', 'static_parser': 'True', 'target_path': 'None', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m12:25:44.281431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3ecd96c0>]}
[0m12:25:44.389472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3f40dde0>]}
[0m12:25:44.390750 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:25:44.692234 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:25:44.914298 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:25:44.915050 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:25:45.003183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3f659ff0>]}
[0m12:25:45.175009 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:25:45.178241 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:25:45.197031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3f65a980>]}
[0m12:25:45.198147 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m12:25:45.198926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3f65aa10>]}
[0m12:25:45.201869 [info ] [MainThread]: 
[0m12:25:45.202631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:25:45.203279 [info ] [MainThread]: 
[0m12:25:45.204254 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:25:45.213359 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m12:25:45.214575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:25:45.602979 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:25:45.963915 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now create_techshop-data-pipeline-2025_staging_marts)
[0m12:25:45.965161 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "staging_marts"
"
[0m12:25:45.983945 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_staging_marts: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_staging_marts"} */
create schema if not exists `techshop-data-pipeline-2025`.`staging_marts`
  
[0m12:25:45.984818 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:25:46.448967 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:15ccd9bc-057c-4a0d-ba8e-98fd29abe04a&page=queryresults
[0m12:25:47.576356 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025_staging_marts'
[0m12:25:47.577433 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:25:47.938676 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_marts, now list_techshop-data-pipeline-2025_staging)
[0m12:25:47.939719 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:25:48.284898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee41783c40>]}
[0m12:25:48.285897 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:25:48.341976 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m12:25:48.343386 [info ] [Thread-1 (]: 1 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m12:25:48.344511 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging, now model.techshop_2025.stg_customers)
[0m12:25:48.345258 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m12:25:48.359036 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m12:25:48.360237 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m12:25:48.399543 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m12:25:48.400956 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m12:25:48.401944 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:49.024696 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:36232c93-0141-4338-9acc-deedbc073b65&page=queryresults
[0m12:25:49.426221 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee21c2a6e0>]}
[0m12:25:49.428118 [info ] [Thread-1 (]: 1 of 4 OK created sql view model staging.stg_customers ......................... [[32mCREATE VIEW (0 processed)[0m in 1.08s]
[0m12:25:49.429396 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m12:25:49.430199 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m12:25:49.431129 [info ] [Thread-1 (]: 2 of 4 START sql view model staging.stg_orders ................................. [RUN]
[0m12:25:49.432298 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m12:25:49.432947 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m12:25:49.442084 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m12:25:49.443239 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m12:25:49.448568 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m12:25:49.449779 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m12:25:49.450706 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:49.985790 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:52cc5cd1-e7c2-4ec4-b9dc-51aa5c43bfde&page=queryresults
[0m12:25:50.330855 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee21c2a6e0>]}
[0m12:25:50.332393 [info ] [Thread-1 (]: 2 of 4 OK created sql view model staging.stg_orders ............................ [[32mCREATE VIEW (0 processed)[0m in 0.90s]
[0m12:25:50.333599 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m12:25:50.334429 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m12:25:50.335514 [info ] [Thread-1 (]: 3 of 4 START sql view model staging.stg_products ............................... [RUN]
[0m12:25:50.336472 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m12:25:50.337509 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m12:25:50.345159 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m12:25:50.346451 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m12:25:50.352054 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m12:25:50.353445 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m12:25:50.354444 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:50.807512 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:4ded32b8-2ebd-4cab-a967-f8c312d61b80&page=queryresults
[0m12:25:51.158645 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee40099810>]}
[0m12:25:51.160430 [info ] [Thread-1 (]: 3 of 4 OK created sql view model staging.stg_products .......................... [[32mCREATE VIEW (0 processed)[0m in 0.82s]
[0m12:25:51.161744 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m12:25:51.163190 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m12:25:51.164304 [info ] [Thread-1 (]: 4 of 4 START sql table model staging_marts.fct_sales ........................... [RUN]
[0m12:25:51.165553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now model.techshop_2025.fct_sales)
[0m12:25:51.166347 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m12:25:51.174615 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m12:25:51.176255 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m12:25:51.227120 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m12:25:51.228664 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`staging_marts`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      -- dbt_project/models/marts/fct_sales.sql


SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
    );
  
[0m12:25:51.229654 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:52.106390 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:80133701-00b2-46ae-be59-e1b1f6dfbc6b&page=queryresults
[0m12:25:58.959821 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a594d27-1348-4c14-bea3-563dc7d80c6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee1ee8a0b0>]}
[0m12:25:58.961277 [info ] [Thread-1 (]: 4 of 4 OK created sql table model staging_marts.fct_sales ...................... [[32mCREATE TABLE (189.7k rows, 19.2 MiB processed)[0m in 7.79s]
[0m12:25:58.962590 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m12:25:58.964996 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:25:59.014790 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:25:59.015564 [debug] [MainThread]: Connection 'create_techshop-data-pipeline-2025_staging_marts' was properly closed.
[0m12:25:59.016135 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m12:25:59.016825 [info ] [MainThread]: 
[0m12:25:59.017579 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 13.81 seconds (13.81s).
[0m12:25:59.019494 [debug] [MainThread]: Command end result
[0m12:25:59.064617 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m12:25:59.067823 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m12:25:59.079234 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m12:25:59.079992 [info ] [MainThread]: 
[0m12:25:59.081364 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:25:59.082149 [info ] [MainThread]: 
[0m12:25:59.082943 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m12:25:59.084672 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.510918, "process_in_blocks": "0", "process_kernel_time": 0.645845, "process_mem_max_rss": "389240", "process_out_blocks": "2336", "process_user_time": 10.654615}
[0m12:25:59.085584 [debug] [MainThread]: Command `dbt run` succeeded at 12:25:59.085360 after 18.51 seconds
[0m12:25:59.086298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3fdb5c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3f40dde0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ee3fc89a80>]}
[0m12:25:59.087009 [debug] [MainThread]: Flushing usage events
[0m12:25:59.327493 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:52:24.531376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aacbe9e6fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aacbd781630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aacbd7815d0>]}


============================== 12:52:24.536001 | 25947607-41ca-4cf8-87bb-ec438d5b8a3c ==============================
[0m12:52:24.536001 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:52:24.537120 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'target_path': 'None', 'debug': 'False', 'partial_parse': 'True', 'no_print': 'None', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt ls', 'profiles_dir': '/home/bquser/.dbt', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'version_check': 'True', 'quiet': 'False', 'write_json': 'True', 'printer_width': '80', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'warn_error': 'None', 'empty': 'None', 'use_colors': 'True', 'log_cache_events': 'False'}
[0m12:52:28.168266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25947607-41ca-4cf8-87bb-ec438d5b8a3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aacbd71b580>]}
[0m12:52:28.279449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25947607-41ca-4cf8-87bb-ec438d5b8a3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aaca0621840>]}
[0m12:52:28.280716 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:52:28.586096 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:52:28.811499 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:52:28.812769 [debug] [MainThread]: Partial parsing: updated file: techshop_2025://models/marts/fct_sales.sql
[0m12:52:29.127739 [error] [MainThread]: Encountered an error:
Compilation Error in model fct_sales (models/marts/fct_sales.sql)
  unexpected '}'
    line 6
      ) }
[0m12:52:29.129623 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": false, "command_wall_clock_time": 4.6928062, "process_in_blocks": "0", "process_kernel_time": 0.630727, "process_mem_max_rss": "378052", "process_out_blocks": "16", "process_user_time": 8.445516}
[0m12:52:29.130864 [debug] [MainThread]: Command `dbt ls` failed at 12:52:29.130628 after 4.69 seconds
[0m12:52:29.131641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aacbe9e6fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aac9ce4b040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aac9ce4b1c0>]}
[0m12:52:29.132389 [debug] [MainThread]: Flushing usage events
[0m12:52:29.355766 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:56:27.605910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745fb22b3490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745fb106fd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745fb106fdf0>]}


============================== 12:56:27.610467 | f3439baf-6b6a-43f8-81a2-dcb00e01a6f9 ==============================
[0m12:56:27.610467 [info ] [MainThread]: Running with dbt=1.10.11
[0m12:56:27.611478 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt ls', 'cache_selected_only': 'False', 'static_parser': 'True', 'debug': 'False', 'target_path': 'None', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bquser/.dbt', 'fail_fast': 'False', 'introspect': 'True', 'quiet': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'empty': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'use_colors': 'True'}
[0m12:56:31.180409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f3439baf-6b6a-43f8-81a2-dcb00e01a6f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745f9128c6d0>]}
[0m12:56:31.289775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f3439baf-6b6a-43f8-81a2-dcb00e01a6f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745fb171b5e0>]}
[0m12:56:31.291132 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:56:31.605671 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m12:56:31.826372 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:56:31.827590 [debug] [MainThread]: Partial parsing: updated file: techshop_2025://models/marts/fct_sales.sql
[0m12:56:32.148619 [error] [MainThread]: Encountered an error:
Compilation Error in model fct_sales (models/marts/fct_sales.sql)
  unexpected '}', expected ')'
    line 6
      }}
[0m12:56:32.150522 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": false, "command_wall_clock_time": 4.636979, "process_in_blocks": "0", "process_kernel_time": 0.549679, "process_mem_max_rss": "377904", "process_out_blocks": "24", "process_user_time": 8.423494}
[0m12:56:32.151795 [debug] [MainThread]: Command `dbt ls` failed at 12:56:32.151563 after 4.64 seconds
[0m12:56:32.152630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745fb22b3490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745f9071f670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745f9071f7f0>]}
[0m12:56:32.153405 [debug] [MainThread]: Flushing usage events
[0m12:56:32.380449 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:00:01.434441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a37978a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a378166e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a368c6110>]}


============================== 13:00:01.439462 | d2b4343c-cb73-41af-b2ab-1ca1ba15b1e1 ==============================
[0m13:00:01.439462 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:00:01.440650 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'indirect_selection': 'eager', 'introspect': 'True', 'write_json': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'log_format': 'default', 'use_colors': 'True', 'profiles_dir': '/home/bquser/.dbt', 'static_parser': 'True', 'partial_parse': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'printer_width': '80', 'version_check': 'True', 'invocation_command': 'dbt ls', 'use_experimental_parser': 'False', 'quiet': 'False', 'debug': 'False', 'warn_error': 'None'}
[0m13:00:05.075407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2b4343c-cb73-41af-b2ab-1ca1ba15b1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a392d0fa0>]}
[0m13:00:05.181137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2b4343c-cb73-41af-b2ab-1ca1ba15b1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a16ab4130>]}
[0m13:00:05.182347 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:00:05.489563 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:00:05.714790 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:00:05.716044 [debug] [MainThread]: Partial parsing: updated file: techshop_2025://models/marts/fct_sales.sql
[0m13:00:06.036958 [error] [MainThread]: Encountered an error:
Compilation Error in model fct_sales (models/marts/fct_sales.sql)
  unexpected '}'
    line 6
      )}
[0m13:00:06.038775 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": false, "command_wall_clock_time": 4.705447, "process_in_blocks": "0", "process_kernel_time": 0.56581, "process_mem_max_rss": "378100", "process_out_blocks": "16", "process_user_time": 8.572799}
[0m13:00:06.039987 [debug] [MainThread]: Command `dbt ls` failed at 13:00:06.039776 after 4.71 seconds
[0m13:00:06.040672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a37978a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a15f0d750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c7a15f0d5d0>]}
[0m13:00:06.041356 [debug] [MainThread]: Flushing usage events
[0m13:00:06.265374 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:01:18.324458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed239c6a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed228050c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed22805060>]}


============================== 13:01:18.328915 | bc64bb80-5b94-42dd-8875-81fd7c12edbe ==============================
[0m13:01:18.328915 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:01:18.329967 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'version_check': 'True', 'warn_error': 'None', 'target_path': 'None', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/bquser/.dbt', 'debug': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt ls', 'static_parser': 'True', 'no_print': 'None', 'introspect': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'write_json': 'True', 'log_cache_events': 'False', 'log_format': 'default', 'empty': 'None'}
[0m13:01:21.856048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc64bb80-5b94-42dd-8875-81fd7c12edbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed057b1d80>]}
[0m13:01:21.961979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc64bb80-5b94-42dd-8875-81fd7c12edbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed229fec50>]}
[0m13:01:21.963202 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:01:22.291640 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:01:22.523798 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:01:22.524599 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:01:22.611865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc64bb80-5b94-42dd-8875-81fd7c12edbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed232e8130>]}
[0m13:01:22.782476 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:01:22.785603 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:01:22.802852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc64bb80-5b94-42dd-8875-81fd7c12edbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed232ea140>]}
[0m13:01:22.803898 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:01:22.804667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc64bb80-5b94-42dd-8875-81fd7c12edbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed232ea020>]}
[0m13:01:22.806329 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m13:01:22.807039 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m13:01:22.807699 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m13:01:22.808332 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m13:01:22.808951 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m13:01:22.809708 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m13:01:22.810413 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m13:01:22.811159 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m13:01:22.811887 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m13:01:22.812549 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m13:01:22.813210 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m13:01:22.813837 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m13:01:22.814500 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m13:01:22.815153 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m13:01:22.815784 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m13:01:22.816493 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m13:01:22.817636 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m13:01:22.818373 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m13:01:22.819242 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m13:01:22.819871 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m13:01:22.821517 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 4.5912404, "process_in_blocks": "0", "process_kernel_time": 0.518458, "process_mem_max_rss": "376596", "process_out_blocks": "1144", "process_user_time": 8.612377}
[0m13:01:22.822743 [debug] [MainThread]: Command `dbt ls` succeeded at 13:01:22.822525 after 4.59 seconds
[0m13:01:22.823676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed239c6a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed057b1d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ed2328f2b0>]}
[0m13:01:22.824441 [debug] [MainThread]: Flushing usage events
[0m13:01:23.058145 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:05:48.592464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4c02bb370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4bf06bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4bf06bf70>]}


============================== 13:05:48.597114 | 6efb98e1-7858-44d5-9611-affdbc9f347b ==============================
[0m13:05:48.597114 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:05:48.598186 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'printer_width': '80', 'cache_selected_only': 'False', 'version_check': 'True', 'invocation_command': 'dbt ls', 'fail_fast': 'False', 'use_colors': 'True', 'profiles_dir': '/home/bquser/.dbt', 'quiet': 'False', 'no_print': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'empty': 'None', 'use_experimental_parser': 'False', 'warn_error': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'log_format': 'default', 'indirect_selection': 'eager', 'partial_parse': 'True'}
[0m13:05:52.188140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6efb98e1-7858-44d5-9611-affdbc9f347b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4bf7181f0>]}
[0m13:05:52.295264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6efb98e1-7858-44d5-9611-affdbc9f347b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4bf73eb90>]}
[0m13:05:52.296512 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:05:52.604237 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:05:52.828968 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:05:52.830242 [debug] [MainThread]: Partial parsing: updated file: techshop_2025://models/marts/fct_sales.sql
[0m13:05:53.308866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6efb98e1-7858-44d5-9611-affdbc9f347b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea49e799f60>]}
[0m13:05:53.478360 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:05:53.481443 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:05:53.498014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6efb98e1-7858-44d5-9611-affdbc9f347b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea49e788070>]}
[0m13:05:53.499111 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:05:53.499784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6efb98e1-7858-44d5-9611-affdbc9f347b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea49e78ad10>]}
[0m13:05:53.501537 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m13:05:53.502233 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m13:05:53.502778 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m13:05:53.503376 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m13:05:53.503903 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m13:05:53.504499 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m13:05:53.505104 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m13:05:53.505662 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m13:05:53.506346 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m13:05:53.507298 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m13:05:53.507913 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m13:05:53.508503 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m13:05:53.509039 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m13:05:53.509617 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m13:05:53.510287 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m13:05:53.510842 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m13:05:53.511568 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m13:05:53.512157 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m13:05:53.512702 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m13:05:53.513311 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m13:05:53.514851 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 5.0168085, "process_in_blocks": "0", "process_kernel_time": 0.536076, "process_mem_max_rss": "381152", "process_out_blocks": "2224", "process_user_time": 8.89156}
[0m13:05:53.515970 [debug] [MainThread]: Command `dbt ls` succeeded at 13:05:53.515769 after 5.02 seconds
[0m13:05:53.516761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4c02bb370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea4bf7181f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea49e7e7ac0>]}
[0m13:05:53.517497 [debug] [MainThread]: Flushing usage events
[0m13:05:53.739587 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:06:35.712752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078ac7a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad077810880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad077810670>]}


============================== 13:06:35.717327 | 5e009223-0208-43bb-9390-c115f89d91b3 ==============================
[0m13:06:35.717327 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:06:35.718417 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'static_parser': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'cache_selected_only': 'False', 'empty': 'False', 'no_print': 'None', 'use_colors': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'quiet': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'target_path': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/bquser/.dbt'}
[0m13:06:39.339868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad07a0309d0>]}
[0m13:06:39.451430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad057a82590>]}
[0m13:06:39.452644 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:06:39.771740 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:06:39.992818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:06:39.993588 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:06:40.080173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078150130>]}
[0m13:06:40.247223 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:06:40.250476 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:06:40.269724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad0782d40d0>]}
[0m13:06:40.270753 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:06:40.271529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad0782fafe0>]}
[0m13:06:40.274669 [info ] [MainThread]: 
[0m13:06:40.275486 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:06:40.276158 [info ] [MainThread]: 
[0m13:06:40.277309 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:06:40.285651 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m13:06:40.287236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:06:40.678930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:06:41.065742 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now create_techshop-data-pipeline-2025_staging_marts)
[0m13:06:41.067330 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "staging_marts"
"
[0m13:06:41.086413 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_staging_marts: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_staging_marts"} */
create schema if not exists `techshop-data-pipeline-2025`.`staging_marts`
  
[0m13:06:41.087268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:06:41.663716 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:0fa09b81-bb47-446a-8786-6e58b81cbd90&page=queryresults
[0m13:06:42.581500 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_techshop-data-pipeline-2025_staging_marts, now list_techshop-data-pipeline-2025_staging_marts)
[0m13:06:42.582638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:06:42.931043 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_marts, now list_techshop-data-pipeline-2025_staging)
[0m13:06:42.932009 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:06:43.271281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad057af2110>]}
[0m13:06:43.272500 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:06:43.327594 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m13:06:43.328932 [info ] [Thread-1 (]: 1 of 4 START sql view model staging.stg_customers .............................. [RUN]
[0m13:06:43.330157 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging, now model.techshop_2025.stg_customers)
[0m13:06:43.330927 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m13:06:43.345753 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m13:06:43.347053 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m13:06:43.386379 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m13:06:43.387890 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m13:06:43.388850 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:06:43.965655 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:421d3641-5a80-4e2e-a75c-9d1bd8befd56&page=queryresults
[0m13:06:44.361711 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad0781a8430>]}
[0m13:06:44.363158 [info ] [Thread-1 (]: 1 of 4 OK created sql view model staging.stg_customers ......................... [[32mCREATE VIEW (0 processed)[0m in 1.03s]
[0m13:06:44.364386 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m13:06:44.365168 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m13:06:44.366321 [info ] [Thread-1 (]: 2 of 4 START sql view model staging.stg_orders ................................. [RUN]
[0m13:06:44.367354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m13:06:44.368106 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m13:06:44.375583 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m13:06:44.376859 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m13:06:44.382773 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m13:06:44.384165 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m13:06:44.385479 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:06:45.012685 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:2665087d-6dc3-47bd-86d2-0716bbdb5a59&page=queryresults
[0m13:06:45.366841 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078b23340>]}
[0m13:06:45.368396 [info ] [Thread-1 (]: 2 of 4 OK created sql view model staging.stg_orders ............................ [[32mCREATE VIEW (0 processed)[0m in 1.00s]
[0m13:06:45.369730 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m13:06:45.370682 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m13:06:45.372171 [info ] [Thread-1 (]: 3 of 4 START sql view model staging.stg_products ............................... [RUN]
[0m13:06:45.373323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m13:06:45.374090 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m13:06:45.382149 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m13:06:45.383451 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m13:06:45.389093 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m13:06:45.390473 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m13:06:45.391606 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:06:45.921865 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:7a2aa414-191f-4085-928e-4b85d1ae15d2&page=queryresults
[0m13:06:46.238487 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078b23340>]}
[0m13:06:46.239946 [info ] [Thread-1 (]: 3 of 4 OK created sql view model staging.stg_products .......................... [[32mCREATE VIEW (0 processed)[0m in 0.86s]
[0m13:06:46.241182 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m13:06:46.242732 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m13:06:46.244314 [info ] [Thread-1 (]: 4 of 4 START sql table model staging_marts.fct_sales ........................... [RUN]
[0m13:06:46.245313 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now model.techshop_2025.fct_sales)
[0m13:06:46.246016 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m13:06:46.253770 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m13:06:46.255045 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m13:06:46.304783 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m13:06:46.306272 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`staging_marts`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      -- dbt_project/models/marts/fct_sales.sql
--


SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
    );
  
[0m13:06:46.307340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:06:47.129948 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:9fbfc3dc-106d-448e-a107-7c959fa20b2c&page=queryresults
[0m13:06:53.734948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e009223-0208-43bb-9390-c115f89d91b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad054a18970>]}
[0m13:06:53.737475 [info ] [Thread-1 (]: 4 of 4 OK created sql table model staging_marts.fct_sales ...................... [[32mCREATE TABLE (189.7k rows, 19.2 MiB processed)[0m in 7.49s]
[0m13:06:53.738918 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m13:06:53.742334 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:06:53.800694 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:06:53.801662 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m13:06:53.802716 [info ] [MainThread]: 
[0m13:06:53.803589 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 13.53 seconds (13.53s).
[0m13:06:53.805728 [debug] [MainThread]: Command end result
[0m13:06:53.854147 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:06:53.858163 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:06:53.871333 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m13:06:53.872144 [info ] [MainThread]: 
[0m13:06:53.873142 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:06:53.873967 [info ] [MainThread]: 
[0m13:06:53.875407 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m13:06:53.878937 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.271126, "process_in_blocks": "0", "process_kernel_time": 0.605076, "process_mem_max_rss": "388916", "process_out_blocks": "2344", "process_user_time": 10.72971}
[0m13:06:53.880368 [debug] [MainThread]: Command `dbt run` succeeded at 13:06:53.880103 after 18.27 seconds
[0m13:06:53.881502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078ac7a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad078901990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad0575b42e0>]}
[0m13:06:53.882909 [debug] [MainThread]: Flushing usage events
[0m13:06:54.126551 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:12:45.294596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e56c01f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e45d9de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e45da9b0>]}


============================== 13:12:45.299821 | 07055776-4e7a-4459-a183-0ba1cffcc7b2 ==============================
[0m13:12:45.299821 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:12:45.300982 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'printer_width': '80', 'cache_selected_only': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'write_json': 'True', 'empty': 'False', 'version_check': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'debug': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select fct_sales --debug', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/home/bquser/.dbt', 'no_print': 'None', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None'}
[0m13:12:49.304989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101c45cd2a0>]}
[0m13:12:49.415363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101c474aa70>]}
[0m13:12:49.416760 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:12:49.736647 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:12:50.001479 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:12:50.002486 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:12:50.096157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e50985e0>]}
[0m13:12:50.286157 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:12:50.290669 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:12:50.313523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e521f820>]}
[0m13:12:50.314713 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:12:50.315530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e5044bb0>]}
[0m13:12:50.318543 [info ] [MainThread]: 
[0m13:12:50.319460 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:12:50.320203 [info ] [MainThread]: 
[0m13:12:50.321314 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:12:50.323796 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m13:12:50.325199 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:12:50.734836 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now list_techshop-data-pipeline-2025_staging_marts)
[0m13:12:50.736466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:12:51.116493 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging_marts, now list_techshop-data-pipeline-2025_staging)
[0m13:12:51.118582 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:12:51.454265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e6e52e60>]}
[0m13:12:51.455662 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:12:51.513886 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m13:12:51.515282 [info ] [Thread-1 (]: 1 of 1 START sql table model staging_marts.fct_sales ........................... [RUN]
[0m13:12:51.516336 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_staging, now model.techshop_2025.fct_sales)
[0m13:12:51.517225 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m13:12:51.535769 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m13:12:51.537331 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m13:12:51.561223 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:12:52.006726 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m13:12:52.008341 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`staging_marts`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      -- dbt_project/models/marts/fct_sales.sql
--


SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
    );
  
[0m13:12:52.725789 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:8d809fe9-7212-490f-b08f-23829af7dd69&page=queryresults
[0m13:12:59.623141 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07055776-4e7a-4459-a183-0ba1cffcc7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101c753d6c0>]}
[0m13:12:59.624906 [info ] [Thread-1 (]: 1 of 1 OK created sql table model staging_marts.fct_sales ...................... [[32mCREATE TABLE (189.7k rows, 19.2 MiB processed)[0m in 8.10s]
[0m13:12:59.626280 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m13:12:59.628693 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:12:59.681930 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:12:59.682920 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m13:12:59.683716 [info ] [MainThread]: 
[0m13:12:59.684422 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.36 seconds (9.36s).
[0m13:12:59.685625 [debug] [MainThread]: Command end result
[0m13:12:59.727667 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:12:59.731055 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:12:59.742996 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m13:12:59.744329 [info ] [MainThread]: 
[0m13:12:59.745407 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:12:59.746402 [info ] [MainThread]: 
[0m13:12:59.747159 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m13:12:59.749203 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.5670185, "process_in_blocks": "0", "process_kernel_time": 1.017248, "process_mem_max_rss": "388988", "process_out_blocks": "2256", "process_user_time": 9.913288}
[0m13:12:59.750443 [debug] [MainThread]: Command `dbt run` succeeded at 13:12:59.750228 after 14.57 seconds
[0m13:12:59.751435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101e56c01f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101c429c580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7101c4787730>]}
[0m13:12:59.752527 [debug] [MainThread]: Flushing usage events
[0m13:12:59.961116 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:27:03.162310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bd0d5a2080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bd0c578730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bd0c5786d0>]}


============================== 13:27:03.167033 | 46ffaee9-0882-49af-9ad5-0dacbf357d69 ==============================
[0m13:27:03.167033 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:27:03.168005 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'printer_width': '80', 'write_json': 'True', 'target_path': 'None', 'version_check': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'invocation_command': 'dbt ls', 'quiet': 'False', 'static_parser': 'True', 'profiles_dir': '/home/bquser/.dbt', 'use_colors': 'True', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'no_print': 'None', 'introspect': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'debug': 'False', 'indirect_selection': 'eager'}
[0m13:27:06.676562 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "techshop_2025", target "dev" invalid: Runtime Error
    Must specify schema
[0m13:27:06.678686 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": false, "command_wall_clock_time": 3.612679, "process_in_blocks": "0", "process_kernel_time": 0.55984, "process_mem_max_rss": "363516", "process_out_blocks": "8", "process_user_time": 7.552299}
[0m13:27:06.680005 [debug] [MainThread]: Command `dbt ls` failed at 13:27:06.679761 after 3.61 seconds
[0m13:27:06.680764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bd0d5a2080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bcec65c1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bcec65c100>]}
[0m13:27:06.681592 [debug] [MainThread]: Flushing usage events
[0m13:27:06.905646 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:31:07.925521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0970ca4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096fbb3be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096fbb36d0>]}


============================== 13:31:07.930364 | 94190319-f192-48af-9db4-a8f697ce1762 ==============================
[0m13:31:07.930364 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:31:07.931481 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'warn_error': 'None', 'version_check': 'True', 'quiet': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/home/bquser/.dbt', 'use_colors': 'True', 'log_format': 'default', 'static_parser': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'write_json': 'True', 'invocation_command': 'dbt ls', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False'}
[0m13:31:11.924980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094fdfba00>]}
[0m13:31:12.064482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094fdfa230>]}
[0m13:31:12.065922 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:31:12.530307 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:31:12.884976 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m13:31:12.886864 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:31:12.887994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09702bf790>]}
[0m13:31:15.955039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094e97aa10>]}
[0m13:31:16.144544 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:31:16.147873 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:31:16.169809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094e978910>]}
[0m13:31:16.170906 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:31:16.171888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94190319-f192-48af-9db4-a8f697ce1762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094e97a830>]}
[0m13:31:16.173747 [info ] [MainThread]: techshop_2025.marts.fct_sales
[0m13:31:16.174526 [info ] [MainThread]: techshop_2025.staging.stg_customers
[0m13:31:16.175452 [info ] [MainThread]: techshop_2025.staging.stg_orders
[0m13:31:16.176405 [info ] [MainThread]: techshop_2025.staging.stg_products
[0m13:31:16.177138 [info ] [MainThread]: source:techshop_2025.raw_data.customers
[0m13:31:16.177822 [info ] [MainThread]: source:techshop_2025.raw_data.marketing_campaigns
[0m13:31:16.178589 [info ] [MainThread]: source:techshop_2025.raw_data.order_items
[0m13:31:16.179461 [info ] [MainThread]: source:techshop_2025.raw_data.orders
[0m13:31:16.180187 [info ] [MainThread]: source:techshop_2025.raw_data.products
[0m13:31:16.180980 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_customers_customer_id
[0m13:31:16.181687 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id
[0m13:31:16.182546 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_order_id
[0m13:31:16.183493 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_order_items_product_id
[0m13:31:16.184224 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_customer_id
[0m13:31:16.184856 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_orders_order_id
[0m13:31:16.185449 [info ] [MainThread]: techshop_2025.source_not_null_raw_data_products_product_id
[0m13:31:16.186033 [info ] [MainThread]: techshop_2025.source_unique_raw_data_customers_customer_id
[0m13:31:16.186650 [info ] [MainThread]: techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id
[0m13:31:16.187274 [info ] [MainThread]: techshop_2025.source_unique_raw_data_orders_order_id
[0m13:31:16.187857 [info ] [MainThread]: techshop_2025.source_unique_raw_data_products_product_id
[0m13:31:16.189491 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 8.377525, "process_in_blocks": "0", "process_kernel_time": 0.706802, "process_mem_max_rss": "385868", "process_out_blocks": "2200", "process_user_time": 12.116773}
[0m13:31:16.190358 [debug] [MainThread]: Command `dbt ls` succeeded at 13:31:16.190179 after 8.38 seconds
[0m13:31:16.190999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0970ca4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0952a28160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f094eba7a00>]}
[0m13:31:16.191705 [debug] [MainThread]: Flushing usage events
[0m13:31:16.443446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:33:07.518376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3348cdfc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3347a34070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3347a34910>]}


============================== 13:33:07.523035 | 26856c11-eb31-49e7-a81f-8d4153126ccc ==============================
[0m13:33:07.523035 [info ] [MainThread]: Running with dbt=1.10.11
[0m13:33:07.524291 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'printer_width': '80', 'debug': 'False', 'cache_selected_only': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'introspect': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run', 'use_colors': 'True', 'no_print': 'None', 'version_check': 'True', 'quiet': 'False', 'target_path': 'None', 'profiles_dir': '/home/bquser/.dbt'}
[0m13:33:11.177649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a334a240be0>]}
[0m13:33:11.288103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a33479efac0>]}
[0m13:33:11.289425 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:33:11.613822 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m13:33:11.837455 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:33:11.838534 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:33:11.927714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3348374130>]}
[0m13:33:12.096136 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:33:12.099156 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:33:12.118485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a33484fab30>]}
[0m13:33:12.119518 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m13:33:12.120334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a33484f8880>]}
[0m13:33:12.123373 [info ] [MainThread]: 
[0m13:33:12.124341 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:33:12.124945 [info ] [MainThread]: 
[0m13:33:12.125967 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:33:12.133695 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025'
[0m13:33:12.135135 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:12.526384 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:12.928160 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025, now create_techshop-data-pipeline-2025_analytics_marts)
[0m13:33:12.929774 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "analytics_marts"
"
[0m13:33:12.948870 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_analytics_marts: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_analytics_marts"} */
create schema if not exists `techshop-data-pipeline-2025`.`analytics_marts`
  
[0m13:33:12.949727 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:13.487675 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:29fccfdb-9811-4463-99c5-cc38917a1953&page=queryresults
[0m13:33:14.388167 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_techshop-data-pipeline-2025_analytics_marts, now create_techshop-data-pipeline-2025_analytics_staging)
[0m13:33:14.389693 [debug] [ThreadPool]: Creating schema "database: "techshop-data-pipeline-2025"
schema: "analytics_staging"
"
[0m13:33:14.393953 [debug] [ThreadPool]: On create_techshop-data-pipeline-2025_analytics_staging: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "create_techshop-data-pipeline-2025_analytics_staging"} */
create schema if not exists `techshop-data-pipeline-2025`.`analytics_staging`
  
[0m13:33:14.394783 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:14.855158 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:3b580cc5-d3a6-4a3d-ad0d-62ee7b1af1cc&page=queryresults
[0m13:33:15.447083 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_techshop-data-pipeline-2025_analytics_staging, now list_techshop-data-pipeline-2025_analytics_marts)
[0m13:33:15.448499 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:15.798116 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_marts, now list_techshop-data-pipeline-2025_analytics_staging)
[0m13:33:15.799250 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:16.157682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3327cd4bb0>]}
[0m13:33:16.159017 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:33:16.213850 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m13:33:16.215005 [info ] [Thread-1 (]: 1 of 4 START sql view model analytics_staging.stg_customers .................... [RUN]
[0m13:33:16.216105 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_staging, now model.techshop_2025.stg_customers)
[0m13:33:16.216837 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m13:33:16.230695 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m13:33:16.231909 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m13:33:16.272695 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_customers"
[0m13:33:16.274530 [debug] [Thread-1 (]: On model.techshop_2025.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_customers"} */


  create or replace view `techshop-data-pipeline-2025`.`analytics_staging`.`stg_customers`
  OPTIONS()
  as -- dbt_project/models/staging/stg_customers.sql


SELECT 
  customer_id,
  first_name,
  last_name,
  LOWER(email) as email,
  UPPER(country) as country_code,
  city,
  created_at,
  customer_segment,
  ROUND(lifetime_value, 2) as lifetime_value,
  ROUND(avg_order_value, 2) as avg_order_value,
  order_frequency,
  SPLIT(preferred_categories, ',') as preferred_categories_array,
  is_vip,
  newsletter_subscriber,
  mobile_app_user,
  last_active_date,
  
  -- Calculs dérivés
  DATE_DIFF(CURRENT_DATE(), created_at, DAY) as days_since_signup,
  DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) as days_since_last_active,
  
  -- Segmentation par ancienneté
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 30 THEN 'New (0-30 days)'
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 90 THEN 'Recent (31-90 days)' 
    WHEN DATE_DIFF(CURRENT_DATE(), created_at, DAY) <= 365 THEN 'Established (91-365 days)'
    ELSE 'Veteran (1+ years)'
  END as customer_tenure_segment,
  
  -- Statut d'activité
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 30 THEN 'Active'
    WHEN DATE_DIFF(CURRENT_DATE(), last_active_date, DAY) <= 90 THEN 'At Risk'
    ELSE 'Inactive' 
  END as activity_status

FROM `techshop-data-pipeline-2025`.`raw_data`.`customers`;


[0m13:33:16.275766 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:33:16.936919 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:c3ee37f1-153a-4be0-9f34-17ee8e2bdf3e&page=queryresults
[0m13:33:17.333530 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a332a945900>]}
[0m13:33:17.335224 [info ] [Thread-1 (]: 1 of 4 OK created sql view model analytics_staging.stg_customers ............... [[32mCREATE VIEW (0 processed)[0m in 1.11s]
[0m13:33:17.336450 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m13:33:17.337311 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m13:33:17.338747 [info ] [Thread-1 (]: 2 of 4 START sql view model analytics_staging.stg_orders ....................... [RUN]
[0m13:33:17.339824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m13:33:17.340567 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m13:33:17.347393 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m13:33:17.348580 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m13:33:17.353774 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_orders"
[0m13:33:17.355077 [debug] [Thread-1 (]: On model.techshop_2025.stg_orders: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_orders"} */


  create or replace view `techshop-data-pipeline-2025`.`analytics_staging`.`stg_orders`
  OPTIONS()
  as -- dbt_project/models/staging/stg_orders.sql


SELECT 
  order_id,
  customer_id,
  order_date,
  ROUND(total_amount, 2) as total_amount,
  ROUND(shipping_cost, 2) as shipping_cost,
  ROUND(total_amount - shipping_cost, 2) as subtotal,
  payment_method,
  order_status,
  sales_channel,
  num_items,
  shipped_date,
  delivered_date,
  created_at,
  
  -- Calculs temporels
  DATE_DIFF(shipped_date, order_date, DAY) as days_to_ship,
  DATE_DIFF(delivered_date, shipped_date, DAY) as days_in_transit,
  DATE_DIFF(delivered_date, order_date, DAY) as total_fulfillment_days,
  
  -- Segmentation par montant
  CASE 
    WHEN total_amount < 50 THEN 'Low Value (<$50)'
    WHEN total_amount < 150 THEN 'Medium Value ($50-$150)'
    WHEN total_amount < 300 THEN 'High Value ($150-$300)'
    ELSE 'Premium Value ($300+)'
  END as order_value_segment,
  
  -- Indicateurs temporels 2025
  EXTRACT(YEAR FROM order_date) as order_year,
  EXTRACT(QUARTER FROM order_date) as order_quarter,
  EXTRACT(MONTH FROM order_date) as order_month,
  EXTRACT(DAYOFWEEK FROM order_date) as order_day_of_week,
  FORMAT_DATE('%A', order_date) as order_day_name,
  
  -- Indicateurs saisonniers
  CASE 
    WHEN EXTRACT(MONTH FROM order_date) IN (12, 1, 2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM order_date) IN (3, 4, 5) THEN 'Spring'
    WHEN EXTRACT(MONTH FROM order_date) IN (6, 7, 8) THEN 'Summer'
    ELSE 'Fall'
  END as season,
  
  -- Indicateurs de performance
  shipping_cost = 0 as is_free_shipping,
  order_status = 'Completed' as is_completed,
  sales_channel = 'Mobile App' as is_mobile_order

FROM `techshop-data-pipeline-2025`.`raw_data`.`orders`
WHERE order_date >= '2025-01-01'
  AND order_date <= '2025-12-31';


[0m13:33:17.355961 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:33:17.938351 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:23dc24be-50fa-4ab4-95bd-4e9ad4ac82e9&page=queryresults
[0m13:33:18.473420 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3348377c40>]}
[0m13:33:18.475032 [info ] [Thread-1 (]: 2 of 4 OK created sql view model analytics_staging.stg_orders .................. [[32mCREATE VIEW (0 processed)[0m in 1.13s]
[0m13:33:18.476367 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m13:33:18.477297 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m13:33:18.478536 [info ] [Thread-1 (]: 3 of 4 START sql view model analytics_staging.stg_products ..................... [RUN]
[0m13:33:18.479530 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m13:33:18.480318 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m13:33:18.487733 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m13:33:18.489055 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m13:33:18.495244 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.stg_products"
[0m13:33:18.496787 [debug] [Thread-1 (]: On model.techshop_2025.stg_products: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.stg_products"} */


  create or replace view `techshop-data-pipeline-2025`.`analytics_staging`.`stg_products`
  OPTIONS()
  as -- dbt_project/models/staging/stg_products.sql


SELECT 
  product_id,
  sku,
  product_name,
  category,
  subcategory,
  brand,
  ROUND(price, 2) as price,
  ROUND(cost, 2) as cost,
  ROUND(price - cost, 2) as margin_amount,
  ROUND((price - cost) / price * 100, 2) as margin_percentage,
  stock_quantity,
  ROUND(avg_rating, 2) as avg_rating,
  num_reviews,
  launch_date,
  weight,
  is_eco_friendly,
  is_ai_enabled,
  is_bestseller,
  created_at,
  updated_at,
  
  -- Segmentation par prix
  CASE 
    WHEN price < 50 THEN 'Budget (<$50)'
    WHEN price < 150 THEN 'Mid-Range ($50-$150)'
    WHEN price < 300 THEN 'Premium ($150-$300)'
    ELSE 'Luxury ($300+)'
  END as price_segment,
  
  -- Statut de popularité
  CASE 
    WHEN num_reviews >= 100 AND avg_rating >= 4.5 THEN 'Top Rated'
    WHEN num_reviews >= 50 AND avg_rating >= 4.0 THEN 'Well Rated'
    WHEN num_reviews >= 10 THEN 'Some Reviews'
    ELSE 'New/Few Reviews'
  END as review_status,
  
  -- Indicateurs 2025
  EXTRACT(YEAR FROM launch_date) = 2025 as is_new_2025,
  DATE_DIFF(CURRENT_DATE(), launch_date, DAY) as days_since_launch,
  stock_quantity <= 10 as is_low_stock,
  
  -- Catégorisation avancée
  CASE 
    WHEN is_eco_friendly AND is_ai_enabled THEN 'Eco-AI Product'
    WHEN is_eco_friendly THEN 'Eco-Friendly'
    WHEN is_ai_enabled THEN 'AI-Enabled'
    ELSE 'Standard'
  END as product_innovation_type

FROM `techshop-data-pipeline-2025`.`raw_data`.`products`;


[0m13:33:18.497878 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:33:19.049230 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:66a1c442-dcc8-4e99-abfc-abeb272c033a&page=queryresults
[0m13:33:19.397310 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3348d89540>]}
[0m13:33:19.398739 [info ] [Thread-1 (]: 3 of 4 OK created sql view model analytics_staging.stg_products ................ [[32mCREATE VIEW (0 processed)[0m in 0.92s]
[0m13:33:19.399886 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m13:33:19.401909 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m13:33:19.403028 [info ] [Thread-1 (]: 4 of 4 START sql table model analytics_marts.fct_sales ......................... [RUN]
[0m13:33:19.403965 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now model.techshop_2025.fct_sales)
[0m13:33:19.404677 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m13:33:19.413687 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m13:33:19.414848 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m13:33:19.461023 [debug] [Thread-1 (]: Writing runtime sql for node "model.techshop_2025.fct_sales"
[0m13:33:19.462477 [debug] [Thread-1 (]: On model.techshop_2025.fct_sales: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "node_id": "model.techshop_2025.fct_sales"} */

  
    

    create or replace table `techshop-data-pipeline-2025`.`analytics_marts`.`fct_sales`
      
    
    

    
    OPTIONS()
    as (
      -- dbt_project/models/marts/fct_sales.sql



SELECT 
  o.order_id,
  o.customer_id,
  oi.product_id,
  o.order_date,
  o.order_year,
  o.order_quarter,
  o.order_month,
  o.season,
  o.sales_channel,
  o.payment_method,
  
  -- Informations client
  c.customer_segment,
  c.customer_tenure_segment,
  c.activity_status,
  c.country_code,
  c.is_vip,
  
  -- Informations produit
  p.category,
  p.subcategory,
  p.brand,
  p.price_segment,
  p.product_innovation_type,
  
  -- Métriques de commande
  oi.quantity,
  oi.unit_price,
  oi.total_price,
  o.total_amount as order_total,
  o.shipping_cost,
  o.num_items as total_items_in_order,
  
  -- Métriques dérivées
  ROUND(oi.total_price / o.total_amount * 100, 2) as item_contribution_pct,
  p.cost * oi.quantity as total_cost,
  (oi.unit_price - p.cost) * oi.quantity as total_margin,
  ROUND((oi.unit_price - p.cost) / oi.unit_price * 100, 2) as margin_percentage,
  
  -- Indicateurs
  o.is_completed,
  o.is_free_shipping,
  o.is_mobile_order,
  p.is_eco_friendly,
  p.is_ai_enabled,
  p.is_bestseller,
  
  -- Métriques temporelles
  o.days_to_ship,
  o.total_fulfillment_days

FROM `techshop-data-pipeline-2025`.`analytics_staging`.`stg_orders` o
JOIN `techshop-data-pipeline-2025`.`raw_data`.`order_items` oi 
  ON o.order_id = oi.order_id
JOIN `techshop-data-pipeline-2025`.`analytics_staging`.`stg_customers` c 
  ON o.customer_id = c.customer_id  
JOIN `techshop-data-pipeline-2025`.`analytics_staging`.`stg_products` p 
  ON oi.product_id = p.product_id
WHERE o.is_completed = TRUE
    );
  
[0m13:33:19.463424 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:33:20.305969 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:340235db-938b-4a8c-9820-f589d448b778&page=queryresults
[0m13:33:25.481714 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26856c11-eb31-49e7-a81f-8d4153126ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3327b0e770>]}
[0m13:33:25.483191 [info ] [Thread-1 (]: 4 of 4 OK created sql table model analytics_marts.fct_sales .................... [[32mCREATE TABLE (189.7k rows, 19.2 MiB processed)[0m in 6.08s]
[0m13:33:25.484377 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m13:33:25.486978 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:33:25.537463 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:33:25.538398 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m13:33:25.539185 [info ] [MainThread]: 
[0m13:33:25.539790 [info ] [MainThread]: Finished running 1 table model, 3 view models in 0 hours 0 minutes and 13.41 seconds (13.41s).
[0m13:33:25.541567 [debug] [MainThread]: Command end result
[0m13:33:25.581003 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m13:33:25.583906 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m13:33:25.594379 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m13:33:25.595113 [info ] [MainThread]: 
[0m13:33:25.595930 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:33:25.596620 [info ] [MainThread]: 
[0m13:33:25.597309 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m13:33:25.598841 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.18357, "process_in_blocks": "0", "process_kernel_time": 0.601029, "process_mem_max_rss": "389060", "process_out_blocks": "2344", "process_user_time": 11.017019}
[0m13:33:25.599618 [debug] [MainThread]: Command `dbt run` succeeded at 13:33:25.599458 after 18.18 seconds
[0m13:33:25.600268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3348cdfc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a334a240be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a33277f49a0>]}
[0m13:33:25.600964 [debug] [MainThread]: Flushing usage events
[0m13:33:25.858666 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:43:34.532599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e23b85fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e22b559c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e22b55960>]}


============================== 14:43:34.538117 | d53b4abb-2266-487b-bd51-68d68eaf4e44 ==============================
[0m14:43:34.538117 [info ] [MainThread]: Running with dbt=1.10.11
[0m14:43:34.539352 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error': 'None', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt docs generate', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'profiles_dir': '/home/bquser/.dbt', 'printer_width': '80', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'empty': 'None', 'static_parser': 'True', 'target_path': 'None', 'debug': 'False', 'use_colors': 'True', 'write_json': 'True', 'fail_fast': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'partial_parse': 'True', 'log_format': 'default', 'introspect': 'True', 'use_experimental_parser': 'False'}
[0m14:43:38.975317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e02b361d0>]}
[0m14:43:39.087537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e22d63760>]}
[0m14:43:39.088879 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:43:39.404723 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m14:43:39.642702 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:43:39.643539 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:43:39.731572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e02306590>]}
[0m14:43:39.755405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e235e2110>]}
[0m14:43:39.756578 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m14:43:39.757451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e235e0ca0>]}
[0m14:43:39.761244 [info ] [MainThread]: 
[0m14:43:39.762098 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:43:39.762788 [info ] [MainThread]: 
[0m14:43:39.763964 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:43:39.772441 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025_analytics_marts'
[0m14:43:39.773572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:43:40.208949 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_marts, now list_techshop-data-pipeline-2025_analytics_staging)
[0m14:43:40.210702 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:43:40.564655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd53b4abb-2266-487b-bd51-68d68eaf4e44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e22ca7160>]}
[0m14:43:40.565857 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:43:40.620920 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m14:43:40.622227 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_staging, now model.techshop_2025.stg_customers)
[0m14:43:40.623104 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m14:43:40.642639 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m14:43:40.643995 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m14:43:40.644992 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:40.695606 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m14:43:40.696657 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m14:43:40.697588 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m14:43:40.698533 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m14:43:40.705427 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m14:43:40.706695 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m14:43:40.707540 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:40.757329 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m14:43:40.758299 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m14:43:40.759352 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m14:43:40.760099 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m14:43:40.767088 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m14:43:40.768483 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m14:43:40.769427 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:40.821363 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m14:43:40.822696 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m14:43:40.823991 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b)
[0m14:43:40.825436 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m14:43:40.845946 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b"
[0m14:43:40.847290 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m14:43:40.848205 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:40.898722 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m14:43:40.899826 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m14:43:40.901252 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b, now test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c)
[0m14:43:40.902055 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m14:43:40.909964 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c"
[0m14:43:40.911211 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m14:43:40.911991 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:40.962302 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m14:43:40.963364 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m14:43:40.964631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c, now test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406)
[0m14:43:40.965444 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m14:43:40.973365 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406"
[0m14:43:40.974623 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m14:43:40.975820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.029229 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m14:43:41.030338 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m14:43:41.031480 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406, now test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7)
[0m14:43:41.032264 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m14:43:41.041625 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7"
[0m14:43:41.043051 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m14:43:41.044104 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.095039 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m14:43:41.096152 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m14:43:41.097804 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7, now test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9)
[0m14:43:41.098890 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m14:43:41.107327 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9"
[0m14:43:41.108692 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m14:43:41.109550 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.159619 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m14:43:41.160637 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m14:43:41.161710 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9, now test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8)
[0m14:43:41.162568 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m14:43:41.170429 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8"
[0m14:43:41.171610 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m14:43:41.172443 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.223914 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m14:43:41.224962 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m14:43:41.226055 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8, now test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4)
[0m14:43:41.226836 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m14:43:41.234897 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4"
[0m14:43:41.236252 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m14:43:41.237133 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.287236 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m14:43:41.288237 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m14:43:41.289752 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4, now test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856)
[0m14:43:41.290824 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m14:43:41.302432 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856"
[0m14:43:41.303715 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m14:43:41.304615 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.354615 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m14:43:41.355677 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m14:43:41.356742 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856, now test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b)
[0m14:43:41.357548 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m14:43:41.367257 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b"
[0m14:43:41.368502 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m14:43:41.369400 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.419791 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m14:43:41.420800 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m14:43:41.421984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b, now test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8)
[0m14:43:41.422771 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m14:43:41.430837 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8"
[0m14:43:41.432044 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m14:43:41.432892 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.482902 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m14:43:41.483923 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m14:43:41.485261 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8, now test.techshop_2025.source_unique_raw_data_products_product_id.6029824217)
[0m14:43:41.486008 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m14:43:41.493609 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_products_product_id.6029824217"
[0m14:43:41.494790 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m14:43:41.495720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.546314 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m14:43:41.547398 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m14:43:41.548335 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_products_product_id.6029824217, now model.techshop_2025.fct_sales)
[0m14:43:41.549316 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m14:43:41.556363 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m14:43:41.557552 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m14:43:41.558416 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:41.608357 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m14:43:41.610628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:43:41.611552 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m14:43:41.615539 [debug] [MainThread]: Command end result
[0m14:43:41.794802 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m14:43:41.798206 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m14:43:41.811960 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m14:43:41.826046 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m14:43:41.826869 [info ] [MainThread]: Building catalog
[0m14:43:41.837749 [debug] [ThreadPool]: Acquiring new bigquery connection 'techshop-data-pipeline-2025.information_schema'
[0m14:43:41.877176 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`analytics_staging`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_products')
                            ) or (
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_customers')
                            ) or (
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_orders')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m14:43:41.878812 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:43:42.798569 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:13796d44-4acb-4c1b-93ac-a55a11057020&page=queryresults
[0m14:43:44.349675 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`raw_data`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('products')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('order_items')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('customers')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('orders')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('marketing_campaigns')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m14:43:44.350968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:43:44.931965 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:874dec99-b5c4-46c9-a2db-5c63e05fb26f&page=queryresults
[0m14:43:46.453737 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`analytics_marts`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_marts')
                            and upper(table_name) = upper('fct_sales')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m14:43:46.455159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:43:47.066660 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:2f135114-b2cd-4a91-8ba9-866e94724cf4&page=queryresults
[0m14:43:48.549877 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:43:48.625696 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/catalog.json
[0m14:43:48.669565 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m14:43:48.672699 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m14:43:48.673460 [info ] [MainThread]: Catalog written to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/catalog.json
[0m14:43:48.675645 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 14.240282, "process_in_blocks": "10288", "process_kernel_time": 1.38326, "process_mem_max_rss": "391420", "process_out_blocks": "5888", "process_user_time": 11.025929}
[0m14:43:48.676637 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:43:48.676435 after 14.24 seconds
[0m14:43:48.677306 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:43:48.677890 [debug] [MainThread]: Connection 'techshop-data-pipeline-2025.information_schema' was properly closed.
[0m14:43:48.678916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e23b85fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e22d63760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b3e235e2140>]}
[0m14:43:48.679713 [debug] [MainThread]: Flushing usage events
[0m14:43:48.893438 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:06:32.489379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f561765f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f54fdcbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f54fdcb50>]}


============================== 15:06:32.493970 | 53451d17-7bca-4a83-8cb1-ee30edc95804 ==============================
[0m15:06:32.493970 [info ] [MainThread]: Running with dbt=1.10.11
[0m15:06:32.495098 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'printer_width': '80', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'profiles_dir': '/home/bquser/.dbt', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'fail_fast': 'False', 'quiet': 'False', 'write_json': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'empty': 'None', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'warn_error': 'None', 'no_print': 'None', 'use_colors': 'True', 'debug': 'False', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m15:06:36.200168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f351a0e20>]}
[0m15:06:36.309965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55706e30>]}
[0m15:06:36.311235 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:06:36.623638 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m15:06:36.857723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:06:36.858515 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:06:36.946846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55aa8130>]}
[0m15:06:36.968027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55aaa2c0>]}
[0m15:06:36.969129 [info ] [MainThread]: Found 4 models, 11 data tests, 5 sources, 495 macros
[0m15:06:36.969944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55aaa200>]}
[0m15:06:36.973865 [info ] [MainThread]: 
[0m15:06:36.974694 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:06:36.975452 [info ] [MainThread]: 
[0m15:06:36.976604 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:06:36.985012 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_techshop-data-pipeline-2025_analytics_staging'
[0m15:06:36.986476 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:06:37.410159 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_staging, now list_techshop-data-pipeline-2025_analytics_marts)
[0m15:06:37.412476 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:37.745221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53451d17-7bca-4a83-8cb1-ee30edc95804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f5519d480>]}
[0m15:06:37.746294 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:37.805565 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_customers
[0m15:06:37.806893 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_techshop-data-pipeline-2025_analytics_marts, now model.techshop_2025.stg_customers)
[0m15:06:37.807773 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_customers
[0m15:06:37.825356 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_customers"
[0m15:06:37.826555 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_customers
[0m15:06:37.827399 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:37.877350 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_customers
[0m15:06:37.878301 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_orders
[0m15:06:37.879410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_customers, now model.techshop_2025.stg_orders)
[0m15:06:37.880161 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_orders
[0m15:06:37.886533 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_orders"
[0m15:06:37.887751 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_orders
[0m15:06:37.888638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:37.938397 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_orders
[0m15:06:37.939355 [debug] [Thread-1 (]: Began running node model.techshop_2025.stg_products
[0m15:06:37.940316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_orders, now model.techshop_2025.stg_products)
[0m15:06:37.941292 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.stg_products
[0m15:06:37.947780 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.stg_products"
[0m15:06:37.949048 [debug] [Thread-1 (]: Began executing node model.techshop_2025.stg_products
[0m15:06:37.949931 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.000345 [debug] [Thread-1 (]: Finished running node model.techshop_2025.stg_products
[0m15:06:38.001326 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m15:06:38.002434 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.techshop_2025.stg_products, now test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b)
[0m15:06:38.003525 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m15:06:38.023534 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b"
[0m15:06:38.024916 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m15:06:38.026181 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.076884 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b
[0m15:06:38.078088 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m15:06:38.079412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_customers_customer_id.5f7576ec6b, now test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c)
[0m15:06:38.080274 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m15:06:38.088857 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c"
[0m15:06:38.090168 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m15:06:38.091054 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.141694 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c
[0m15:06:38.142702 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m15:06:38.144587 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_marketing_campaigns_campaign_id.aed14aab9c, now test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406)
[0m15:06:38.145503 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m15:06:38.153545 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406"
[0m15:06:38.154807 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m15:06:38.155661 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.207501 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406
[0m15:06:38.208431 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m15:06:38.209442 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_order_id.a98cf18406, now test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7)
[0m15:06:38.210184 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m15:06:38.217578 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7"
[0m15:06:38.218765 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m15:06:38.219732 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.272238 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7
[0m15:06:38.273380 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m15:06:38.274326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_order_items_product_id.5ac97549a7, now test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9)
[0m15:06:38.275550 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m15:06:38.290541 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9"
[0m15:06:38.291860 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m15:06:38.292811 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.343263 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9
[0m15:06:38.344299 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m15:06:38.345412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_customer_id.6c7f10cde9, now test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8)
[0m15:06:38.346273 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m15:06:38.354092 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8"
[0m15:06:38.355310 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m15:06:38.356218 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.409372 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8
[0m15:06:38.410337 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m15:06:38.411558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_orders_order_id.840e0f87e8, now test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4)
[0m15:06:38.412421 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m15:06:38.420345 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4"
[0m15:06:38.421660 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m15:06:38.422506 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.473277 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4
[0m15:06:38.474263 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m15:06:38.475373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_not_null_raw_data_products_product_id.135fb6ffe4, now test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856)
[0m15:06:38.476167 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m15:06:38.487182 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856"
[0m15:06:38.488470 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m15:06:38.489350 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.540119 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856
[0m15:06:38.541114 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m15:06:38.542453 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_customers_customer_id.0090285856, now test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b)
[0m15:06:38.543287 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m15:06:38.552730 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b"
[0m15:06:38.553921 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m15:06:38.554773 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.604952 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b
[0m15:06:38.605906 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m15:06:38.606943 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_marketing_campaigns_campaign_id.0354a14a2b, now test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8)
[0m15:06:38.607720 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m15:06:38.615294 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8"
[0m15:06:38.616534 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m15:06:38.617390 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.667867 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8
[0m15:06:38.668825 [debug] [Thread-1 (]: Began running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m15:06:38.669866 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_orders_order_id.9b90266fc8, now test.techshop_2025.source_unique_raw_data_products_product_id.6029824217)
[0m15:06:38.670639 [debug] [Thread-1 (]: Began compiling node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m15:06:38.678125 [debug] [Thread-1 (]: Writing injected SQL for node "test.techshop_2025.source_unique_raw_data_products_product_id.6029824217"
[0m15:06:38.679524 [debug] [Thread-1 (]: Began executing node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m15:06:38.680669 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.731176 [debug] [Thread-1 (]: Finished running node test.techshop_2025.source_unique_raw_data_products_product_id.6029824217
[0m15:06:38.732235 [debug] [Thread-1 (]: Began running node model.techshop_2025.fct_sales
[0m15:06:38.733267 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.techshop_2025.source_unique_raw_data_products_product_id.6029824217, now model.techshop_2025.fct_sales)
[0m15:06:38.734359 [debug] [Thread-1 (]: Began compiling node model.techshop_2025.fct_sales
[0m15:06:38.741616 [debug] [Thread-1 (]: Writing injected SQL for node "model.techshop_2025.fct_sales"
[0m15:06:38.742891 [debug] [Thread-1 (]: Began executing node model.techshop_2025.fct_sales
[0m15:06:38.743788 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.793630 [debug] [Thread-1 (]: Finished running node model.techshop_2025.fct_sales
[0m15:06:38.795877 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:06:38.796726 [debug] [MainThread]: Connection 'model.techshop_2025.fct_sales' was properly closed.
[0m15:06:38.800839 [debug] [MainThread]: Command end result
[0m15:06:38.978323 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m15:06:38.981880 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m15:06:38.994644 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/run_results.json
[0m15:06:38.998572 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m15:06:38.999294 [info ] [MainThread]: Building catalog
[0m15:06:39.010255 [debug] [ThreadPool]: Acquiring new bigquery connection 'techshop-data-pipeline-2025.information_schema'
[0m15:06:39.054174 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`raw_data`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('marketing_campaigns')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('products')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('orders')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('order_items')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('customers')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`raw_data`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m15:06:39.056010 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:06:39.705303 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:b150379a-7b26-43f3-abc3-235076897a5f&page=queryresults
[0m15:06:42.370660 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`analytics_marts`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_marts')
                            and upper(table_name) = upper('fct_sales')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`analytics_marts`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m15:06:42.372053 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:43.170802 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:28204784-c78e-4fae-baae-25cba621180e&page=queryresults
[0m15:06:46.039901 [debug] [ThreadPool]: On techshop-data-pipeline-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.10.11", "profile_name": "techshop_2025", "target_name": "dev", "connection_name": "techshop-data-pipeline-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `techshop-data-pipeline-2025`.`analytics_staging`.__TABLES__ tables
    left join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_orders')
                            ) or (
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_products')
                            ) or (
                                upper(table_schema) = upper('analytics_staging')
                            and upper(table_name) = upper('stg_customers')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.COLUMNS columns
    join `techshop-data-pipeline-2025`.`analytics_staging`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m15:06:46.041501 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:46.642868 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=techshop-data-pipeline-2025&j=bq:US:0de872ae-6cce-4991-99db-eda2dd821861&page=queryresults
[0m15:06:49.989895 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:50.068143 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/catalog.json
[0m15:06:50.113473 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/manifest.json
[0m15:06:50.116636 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/semantic_manifest.json
[0m15:06:50.117525 [info ] [MainThread]: Catalog written to /opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/target/catalog.json
[0m15:06:50.119747 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 17.728132, "process_in_blocks": "0", "process_kernel_time": 0.769669, "process_mem_max_rss": "389732", "process_out_blocks": "5888", "process_user_time": 10.816837}
[0m15:06:50.120778 [debug] [MainThread]: Command `dbt docs generate` succeeded at 15:06:50.120571 after 17.73 seconds
[0m15:06:50.121489 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m15:06:50.122135 [debug] [MainThread]: Connection 'techshop-data-pipeline-2025.information_schema' was properly closed.
[0m15:06:50.122797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f561765f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55aaa8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755f55aaa2c0>]}
[0m15:06:50.123560 [debug] [MainThread]: Flushing usage events
[0m15:06:50.347862 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:09:02.172260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b95ababd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b9598eea10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b9598efd00>]}


============================== 15:09:02.176929 | d07654a3-0092-4ce6-b804-5ea114b1ae96 ==============================
[0m15:09:02.176929 [info ] [MainThread]: Running with dbt=1.10.11
[0m15:09:02.177983 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'partial_parse': 'True', 'log_format': 'default', 'empty': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'profiles_dir': '/home/bquser/.dbt', 'target_path': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'log_path': '/opt/app/bigquery-projects/techshop-data-pipeline-2025/dbt_project/logs', 'invocation_command': 'dbt docs serve --port 8080', 'warn_error': 'None', 'introspect': 'True'}
[0m15:09:05.757235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd07654a3-0092-4ce6-b804-5ea114b1ae96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b95c088d30>]}
[0m15:09:05.867262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd07654a3-0092-4ce6-b804-5ea114b1ae96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b959976920>]}
[0m16:53:59.185350 [error] [MainThread]: Encountered an error:

